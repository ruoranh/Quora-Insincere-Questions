{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text manipulation\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import *\n",
    "import scipy\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import nltk.collocations as collocations\n",
    "from nltk.tag import tnt\n",
    "import spacy\n",
    "\n",
    "# modelling\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of insincere questions: 80810\n",
      "No. of sincere questions: 1225312\n",
      "% of insincere questions: 0.06187017751787352\n",
      "Null score: 0.9381298224821265\n"
     ]
    }
   ],
   "source": [
    "no_insincere = train[train['target']==1].target.count()\n",
    "no_sincere = train[train['target']==0].target.count()\n",
    "\n",
    "print('No. of insincere questions:', no_insincere)\n",
    "print('No. of sincere questions:', no_sincere)\n",
    "print('% of insincere questions:', train.target.mean())\n",
    "print('Null score:', 1- train.target.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions and pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_trans(vectorizer, X_train, X_test):\n",
    "    # can also take a transformer\n",
    "    vect = vectorizer\n",
    "    vect.fit(X_train)\n",
    "    return vect.transform(X_train), vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinominalNB function for printing scores and storing into df.\n",
    "def model_score(model, X_train, X_test, y_train, y_test, score_df, model_label):\n",
    "    estimator = model\n",
    "    estimator.fit(X_train, y_train)\n",
    "    test_score =  estimator.score(X_test, y_test)\n",
    "    f1 = f1_score(y_test, estimator.predict(X_test))\n",
    "    \n",
    "    print('Train Accuracy :', estimator.score(X_train, y_train))\n",
    "    print('Test Accuracy:', test_score)\n",
    "    print('Test F1 score:', f1)\n",
    "    score_df.loc[model_label, 'Test_Accuracy'] = test_score\n",
    "    score_df.loc[model_label, 'Test_F1_score'] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validate function for printing scores and storing into df.\n",
    "def cv_score(model, X, y, model_label,  cv=5, ):    \n",
    "    \n",
    "    # instantiating model\n",
    "    estimator = model\n",
    "    \n",
    "    cv_result = cross_validate(estimator, X, y, cv = cv, n_jobs=-1, scoring=['accuracy', 'f1'])\n",
    "    \n",
    "    print('Test Accuracy Mean:',cv_result['test_accuracy'].mean())\n",
    "    print('Test Accuracy STD:',cv_result['test_accuracy'].std())\n",
    "    print('Test F1:', cv_result['test_f1'].mean())\n",
    "    score_df.loc[model_label, 'CV_Accuracy'] = cv_result['test_accuracy'].mean()\n",
    "    score_df.loc[model_label, 'CV_Acc_STD'] = cv_result['test_accuracy'].std()\n",
    "    score_df.loc[model_label, 'CV_F1_score'] = cv_result['test_f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV function, auto display best score and parameters and storing in df\n",
    "def gridcv(model, X, y, params, cv= 5 ):\n",
    "    \n",
    "    # instantiating model can also be a pipeline\n",
    "    estimator = model\n",
    "    \n",
    "    gridcv = GridSearchCV(estimator=estimator, param_grid=params, cv = cv, verbose=10, n_jobs=6)\n",
    "    gridcv.fit(X, y)\n",
    "    \n",
    "    print(gridcv.best_params_)\n",
    "    print(gridcv.best_score_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(nltk.corpus.stopwords.words('english')) + list(string.punctuation) + [\"''\", '``','’', \"'s\", \"'d\", \"'ll\", \"'t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer pipeline and parameters\n",
    "pipeCVNB = Pipeline([('CV',CountVectorizer(stop_words=stopwords)), \n",
    "                    ('NB',MultinomialNB())])\n",
    "\n",
    "paramsCVNB = {'CV__max_df':(1.0, 0.9, 0.8, 0.7),\n",
    "       'CV__min_df': (1, 2, 0.01 , 0.1, 0.2),\n",
    "         'CV__ngram_range':((1,1), (1,2), (1,3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer pipeline and parameters\n",
    "pipeTVNB = Pipeline([('TV',TfidfVectorizer(stop_words=stopwords)), \n",
    "                    ('NB',MultinomialNB())])\n",
    "\n",
    "paramsTVNB = {'TV__max_df':(1.0, 0.9, 0.8, 0.7, 0.6),\n",
    "       'TV__min_df': (1, 2, 0.01, 0.05, 0.1),\n",
    "         'TV__ngram_range':((1,1), (1,2), (1,3), (2,2), (2,3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spaCy to lemmatize using POS tags in one step, with out converting between WordNet and Treebank tags, using NLTK\n",
    "spac = spacy.load('en', disable=['parser', 'ner'])\n",
    "def lemmatizer(text):\n",
    "    text = spac(text)\n",
    "    return ' '.join([token.lemma_ for token in text if token.lemma_ not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemma_q = [lemmatizer(q) for q in train.question_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quebec nationalist see -PRON- province nation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-PRON- adopt dog would -PRON- encourage people...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>velocity affect time velocity affect space geo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>otto von guericke use magdeburg hemisphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-PRON- convert montra helicon mountain bike ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gaza slowly become auschwitz dachau treblinka ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quora automatically ban conservative opinion r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-PRON- crazy -PRON- wash wipe -PRON- grocery g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>thing dress moderately different dress modestly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-PRON- -PRON- -PRON- ever phase wherein -PRON-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-PRON- say feminism</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>calgary flames found</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dumb yet possibly true explanation trump elect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-PRON- use -PRON- external hard disk os well d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-PRON- 30 live home boyfriend -PRON- would lov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-PRON- know bram fischer rivonia trial</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>difficult -PRON- find good instructor take cla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-PRON- lick skin corpse</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-PRON- think amazon adopt house approach manuf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>many barony may exist within county palatine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-PRON- know whether girl sex sex -PRON-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-PRON- become fast learner -PRON- professional...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>united states become large dictatorship world</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>strange phenomenon -PRON- know witness generat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-PRON- leave -PRON- friend find new one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-PRON- make amazon alexa trigger event browser</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>two democracy never ever go full fledged war s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-PRON- top cbse 6 month</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-PRON- know visit mcleodganj triund trek</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>modern military submarine reduce noise achieve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306092</th>\n",
       "      <td>-PRON- hardly talk -PRON- interest reading his...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306093</th>\n",
       "      <td>-PRON- intimate relation -PRON- cousin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306094</th>\n",
       "      <td>-PRON- singer lyric voice -PRON- head religiou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306095</th>\n",
       "      <td>ginger plant naturally contain sugar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306096</th>\n",
       "      <td>technological advance medicine harm good humanity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306097</th>\n",
       "      <td>-PRON- pass class 11 math -PRON- 85 mark 100 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306098</th>\n",
       "      <td>-PRON- think physical trait -PRON- bear affect...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306099</th>\n",
       "      <td>pakis smell curry shit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306100</th>\n",
       "      <td>quora -PRON- good downvot answer -PRON- vote -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306101</th>\n",
       "      <td>wahabis muslim puritan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306102</th>\n",
       "      <td>step -PRON- take live normal life -PRON- suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306103</th>\n",
       "      <td>trump right usa benevolent towards -PRON- neig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306104</th>\n",
       "      <td>33 late career creative advertising</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306105</th>\n",
       "      <td>difference filteration work -PRON- liver filte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306106</th>\n",
       "      <td>universe pop existence nothing stop -PRON- pop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306107</th>\n",
       "      <td>share service technology team measure -PRON- v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306108</th>\n",
       "      <td>dsatm civil engineering</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306109</th>\n",
       "      <td>-PRON- know problem depend solely boundary geo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306110</th>\n",
       "      <td>comic idea -PRON- tube video shoot alone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306111</th>\n",
       "      <td>-PRON- 10 million bitcoin could -PRON- sell -P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306112</th>\n",
       "      <td>-PRON- ashamed indian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306113</th>\n",
       "      <td>method determine fossil age 10th std</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306114</th>\n",
       "      <td>-PRON- story today</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306115</th>\n",
       "      <td>-PRON- consume 150 gms protein daily vegetaria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306116</th>\n",
       "      <td>good career option msc chemistry student quali...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306117</th>\n",
       "      <td>technical skill -PRON- need computer science u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306118</th>\n",
       "      <td>ms ece good job prospect usa like india job pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306119</th>\n",
       "      <td>foam insulation toxic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306120</th>\n",
       "      <td>one start research project base biochemistry u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306121</th>\n",
       "      <td>win battle wolverine puma</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1306122 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question_text  target\n",
       "0        quebec nationalist see -PRON- province nation ...       0\n",
       "1        -PRON- adopt dog would -PRON- encourage people...       0\n",
       "2        velocity affect time velocity affect space geo...       0\n",
       "3               otto von guericke use magdeburg hemisphere       0\n",
       "4        -PRON- convert montra helicon mountain bike ch...       0\n",
       "5        gaza slowly become auschwitz dachau treblinka ...       0\n",
       "6        quora automatically ban conservative opinion r...       0\n",
       "7        -PRON- crazy -PRON- wash wipe -PRON- grocery g...       0\n",
       "8          thing dress moderately different dress modestly       0\n",
       "9        -PRON- -PRON- -PRON- ever phase wherein -PRON-...       0\n",
       "10                                     -PRON- say feminism       0\n",
       "11                                    calgary flames found       0\n",
       "12          dumb yet possibly true explanation trump elect       0\n",
       "13       -PRON- use -PRON- external hard disk os well d...       0\n",
       "14       -PRON- 30 live home boyfriend -PRON- would lov...       0\n",
       "15                  -PRON- know bram fischer rivonia trial       0\n",
       "16       difficult -PRON- find good instructor take cla...       0\n",
       "17                                 -PRON- lick skin corpse       0\n",
       "18       -PRON- think amazon adopt house approach manuf...       0\n",
       "19            many barony may exist within county palatine       0\n",
       "20                 -PRON- know whether girl sex sex -PRON-       0\n",
       "21       -PRON- become fast learner -PRON- professional...       0\n",
       "22           united states become large dictatorship world       1\n",
       "23       strange phenomenon -PRON- know witness generat...       0\n",
       "24                 -PRON- leave -PRON- friend find new one       0\n",
       "25          -PRON- make amazon alexa trigger event browser       0\n",
       "26       two democracy never ever go full fledged war s...       0\n",
       "27                                 -PRON- top cbse 6 month       0\n",
       "28                -PRON- know visit mcleodganj triund trek       0\n",
       "29       modern military submarine reduce noise achieve...       0\n",
       "...                                                    ...     ...\n",
       "1306092  -PRON- hardly talk -PRON- interest reading his...       0\n",
       "1306093             -PRON- intimate relation -PRON- cousin       1\n",
       "1306094  -PRON- singer lyric voice -PRON- head religiou...       1\n",
       "1306095               ginger plant naturally contain sugar       0\n",
       "1306096  technological advance medicine harm good humanity       0\n",
       "1306097  -PRON- pass class 11 math -PRON- 85 mark 100 1...       0\n",
       "1306098  -PRON- think physical trait -PRON- bear affect...       0\n",
       "1306099                             pakis smell curry shit       1\n",
       "1306100  quora -PRON- good downvot answer -PRON- vote -...       0\n",
       "1306101                             wahabis muslim puritan       0\n",
       "1306102  step -PRON- take live normal life -PRON- suffe...       0\n",
       "1306103  trump right usa benevolent towards -PRON- neig...       1\n",
       "1306104                33 late career creative advertising       0\n",
       "1306105  difference filteration work -PRON- liver filte...       0\n",
       "1306106  universe pop existence nothing stop -PRON- pop...       0\n",
       "1306107  share service technology team measure -PRON- v...       0\n",
       "1306108                            dsatm civil engineering       0\n",
       "1306109  -PRON- know problem depend solely boundary geo...       0\n",
       "1306110           comic idea -PRON- tube video shoot alone       0\n",
       "1306111  -PRON- 10 million bitcoin could -PRON- sell -P...       0\n",
       "1306112                              -PRON- ashamed indian       1\n",
       "1306113               method determine fossil age 10th std       0\n",
       "1306114                                 -PRON- story today       0\n",
       "1306115  -PRON- consume 150 gms protein daily vegetaria...       0\n",
       "1306116  good career option msc chemistry student quali...       0\n",
       "1306117  technical skill -PRON- need computer science u...       0\n",
       "1306118  ms ece good job prospect usa like india job pr...       0\n",
       "1306119                              foam insulation toxic       0\n",
       "1306120  one start research project base biochemistry u...       0\n",
       "1306121                          win battle wolverine puma       0\n",
       "\n",
       "[1306122 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_train = pd.DataFrame(lemma_q, columns = ['question_text'])\n",
    "lemma_train['target'] = train.target\n",
    "lemma_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemma_train.to_pickle('./lemma_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_train = pd.read_pickle('./lemma_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lemma_train.question_text\n",
    "y = lemma_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state = 495)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_raw, X_test_raw =  vect_trans(CountVectorizer(), X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9367031751006287\n",
      "Test Accuracy: 0.9367257626381569\n",
      "Test F1 score: 0.5438972162740899\n",
      "Test Accuracy Mean: 0.9316684201196621\n",
      "Test Accuracy STD: 0.0005027082113257391\n",
      "Test F1: 0.5144468147496293\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.936726</td>\n",
       "      <td>0.543897</td>\n",
       "      <td>0.931668</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.514447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB       0.936726       0.543897     0.931668    0.000503   \n",
       "\n",
       "              CV_F1_score  \n",
       "Raw_token_NB     0.514447  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Raw_token_NB'\n",
    "\n",
    "model_score(model, X_train_raw, X_test_raw, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_raw, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9367031751006287\n",
      "test score: 0.9367257626381569\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_raw, y_train)  \n",
    "test_score =  nb.score(X_test_raw, y_test)\n",
    "print('train score:', nb.score(X_train_raw, y_train))\n",
    "print('test score:', test_score)\n",
    "y_pred = nb.predict(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5438972162740899\n",
      "0.754951028488399\n",
      "0.9367257626381569\n",
      "0.9398883606860133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[293551,  12777],\n",
       "       [  7884,  12319]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f1_score(y_test, y_pred) )\n",
    "print(f1_score(y_test, y_pred, average='macro') )\n",
    "print(f1_score(y_test, y_pred, average='micro') )\n",
    "print(f1_score(y_test, y_pred, average='weighted') )\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_t, X_test_t=  vect_trans(CountVectorizer(max_df=1.0, min_df=1, ngram_range=(1,1), \n",
    "                                                    stop_words=stopwords), X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9366899042559599\n",
      "Test Accuracy: 0.936716575149067\n",
      "Test F1 score: 0.5438209192458828\n",
      "Test Accuracy Mean: 0.9316520867713353\n",
      "Test Accuracy STD: 0.0005082745707158826\n",
      "Test F1: 0.5143515648115624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.936726</td>\n",
       "      <td>0.543897</td>\n",
       "      <td>0.931668</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.514447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936717</td>\n",
       "      <td>0.543821</td>\n",
       "      <td>0.931652</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.514352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB       0.936726       0.543897     0.931668    0.000503   \n",
       "Token_NB           0.936717       0.543821     0.931652    0.000508   \n",
       "\n",
       "              CV_F1_score  \n",
       "Raw_token_NB     0.514447  \n",
       "Token_NB         0.514352  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Token_NB'\n",
    "\n",
    "model_score(model, X_train_t, X_test_t, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_t, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_bi, X_test_bi=  vect_trans(CountVectorizer(ngram_range=(1,2), stop_words=stopwords), X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9644290321164649\n",
      "Test Accuracy: 0.9460020641225488\n",
      "Test F1 score: 0.41823940873696713\n",
      "Test Accuracy Mean: 0.9378036343865782\n",
      "Test Accuracy STD: 0.00016942306706332178\n",
      "Test F1: 0.44701183939132355\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.936726</td>\n",
       "      <td>0.543897</td>\n",
       "      <td>0.931668</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.514447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936717</td>\n",
       "      <td>0.543821</td>\n",
       "      <td>0.931652</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.514352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_NB</th>\n",
       "      <td>0.946002</td>\n",
       "      <td>0.418239</td>\n",
       "      <td>0.937804</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.447012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB       0.936726       0.543897     0.931668    0.000503   \n",
       "Token_NB           0.936717       0.543821     0.931652    0.000508   \n",
       "Bigram_NB          0.946002       0.418239     0.937804    0.000169   \n",
       "\n",
       "              CV_F1_score  \n",
       "Raw_token_NB     0.514447  \n",
       "Token_NB         0.514352  \n",
       "Bigram_NB        0.447012  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Bigram_NB'\n",
    "\n",
    "model_score(model, X_train_bi, X_test_bi, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_bi, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_tri, X_test_tri=  vect_trans(CountVectorizer(ngram_range=(1,3), stop_words=stopwords), X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9810921088495096\n",
      "Test Accuracy: 0.944179878786394\n",
      "Test F1 score: 0.27725127879773187\n",
      "Test Accuracy Mean: 0.939216468581791\n",
      "Test Accuracy STD: 0.0002987832989750409\n",
      "Test F1: 0.4579298632261599\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.936726</td>\n",
       "      <td>0.543897</td>\n",
       "      <td>0.931668</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.514447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936717</td>\n",
       "      <td>0.543821</td>\n",
       "      <td>0.931652</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.514352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_NB</th>\n",
       "      <td>0.946002</td>\n",
       "      <td>0.418239</td>\n",
       "      <td>0.937804</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.447012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_NB</th>\n",
       "      <td>0.944180</td>\n",
       "      <td>0.277251</td>\n",
       "      <td>0.939216</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.457930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB       0.936726       0.543897     0.931668    0.000503   \n",
       "Token_NB           0.936717       0.543821     0.931652    0.000508   \n",
       "Bigram_NB          0.946002       0.418239     0.937804    0.000169   \n",
       "Trigram_NB         0.944180       0.277251     0.939216    0.000299   \n",
       "\n",
       "              CV_F1_score  \n",
       "Raw_token_NB     0.514447  \n",
       "Token_NB         0.514352  \n",
       "Bigram_NB        0.447012  \n",
       "Trigram_NB       0.457930  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Trigram_NB'\n",
    "\n",
    "model_score(model, X_train_tri, X_test_tri, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_tri, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch min/max df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:   54.8s\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=6)]: Done  60 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=6)]: Done  73 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=6)]: Done 101 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=6)]: Done 120 out of 120 | elapsed: 16.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CV__max_df': 1.0, 'CV__min_df': 1, 'CV__ngram_range': (1, 2)}\n",
      "0.9460979122919667\n"
     ]
    }
   ],
   "source": [
    "# params = {'CV__max_df':(1.0, 0.9),\n",
    "#        'CV__min_df': (1, 2, 0.01, 0.02),\n",
    "#         'CV__ngram_range':((1,1), (1,2), (1,3))}\n",
    "\n",
    "# gridcv(pipeCVNB, X_train, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# X_train_t, X_test_t=  vect_trans(CountVectorizer(max_df=1.0, min_df=1, ngram_range=(1,2), \n",
    "#                                                     stop_words=stopwords), X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9744383114993911\n",
      "test score: 0.9471045628133317\n",
      "Test Accuracy Mean: 0.9444359946999341\n",
      "Test Accuracy STD: 0.0004057190754223053\n",
      "Test F1: 0.47813979276026675\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token_NB</th>\n",
       "      <td>0.934414</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.548922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_token_NB</th>\n",
       "      <td>0.937608</td>\n",
       "      <td>0.932782</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.527635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_bi_NB</th>\n",
       "      <td>0.947105</td>\n",
       "      <td>0.944436</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.478140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_bi_RF</th>\n",
       "      <td>0.938128</td>\n",
       "      <td>0.944436</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.478140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_tri_NB</th>\n",
       "      <td>0.944734</td>\n",
       "      <td>0.944596</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.505501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_tri_RF</th>\n",
       "      <td>0.938128</td>\n",
       "      <td>0.944596</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.505501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_df_NB</th>\n",
       "      <td>0.947105</td>\n",
       "      <td>0.944436</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.478140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Test_score  CV_Accuracy  CV_Acc_STD  F1_score\n",
       "token_NB         0.934414     0.932161    0.000523  0.548922\n",
       "stop_token_NB    0.937608     0.932782    0.000462  0.527635\n",
       "stop_bi_NB       0.947105     0.944436    0.000406  0.478140\n",
       "stop_bi_RF       0.938128     0.944436    0.000406  0.478140\n",
       "stop_tri_NB      0.944734     0.944596    0.000425  0.505501\n",
       "stop_tri_RF      0.938128     0.944596    0.000425  0.505501\n",
       "stop_df_NB       0.947105     0.944436    0.000406  0.478140"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = MultinomialNB()\n",
    "# model_label = 'Grid_DFNB'\n",
    "\n",
    "# model_score(model, X_train_t, X_test_t, y_train, y_test, score_df, model_label)\n",
    "# cv_score(model, X_train_t, y_train, model_label)\n",
    "# score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf_t, X_test_tf_t = vect_trans(TfidfTransformer(), X_train_t, X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.941965575428929\n",
      "Test Accuracy: 0.9408693202176822\n",
      "Test F1 score: 0.11746960416857119\n",
      "Test Accuracy Mean: 0.939631948814068\n",
      "Test Accuracy STD: 0.00010638318001755925\n",
      "Test F1: 0.0881597738101094\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.936726</td>\n",
       "      <td>0.543897</td>\n",
       "      <td>0.931668</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.514447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936717</td>\n",
       "      <td>0.543821</td>\n",
       "      <td>0.931652</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.514352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_NB</th>\n",
       "      <td>0.946002</td>\n",
       "      <td>0.418239</td>\n",
       "      <td>0.937804</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.447012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_NB</th>\n",
       "      <td>0.944180</td>\n",
       "      <td>0.277251</td>\n",
       "      <td>0.939216</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.457930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf_t_NB</th>\n",
       "      <td>0.940869</td>\n",
       "      <td>0.117470</td>\n",
       "      <td>0.939632</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.088160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB       0.936726       0.543897     0.931668    0.000503   \n",
       "Token_NB           0.936717       0.543821     0.931652    0.000508   \n",
       "Bigram_NB          0.946002       0.418239     0.937804    0.000169   \n",
       "Trigram_NB         0.944180       0.277251     0.939216    0.000299   \n",
       "Tfidf_t_NB         0.940869       0.117470     0.939632    0.000106   \n",
       "\n",
       "              CV_F1_score  \n",
       "Raw_token_NB     0.514447  \n",
       "Token_NB         0.514352  \n",
       "Bigram_NB        0.447012  \n",
       "Trigram_NB       0.457930  \n",
       "Tfidf_t_NB       0.088160  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Tfidf_t_NB'\n",
    "\n",
    "model_score(model, X_train_tf_t , X_test_tf_t, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_tf_t, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Best Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one list of all question tokens\n",
    "full_text = []\n",
    "\n",
    "for text in X_train:\n",
    "    full_text += [w for w in nltk.word_tokenize(text.lower()) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7226905"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-pron-', '-pron-'),\n",
       " ('-pron-', 'get'),\n",
       " ('united', 'states'),\n",
       " ('year', 'old'),\n",
       " ('good', 'way'),\n",
       " ('donald', 'trump'),\n",
       " ('-pron-', 'think'),\n",
       " ('would', '-pron-'),\n",
       " ('-pron-', 'want'),\n",
       " ('-pron-', 'possible'),\n",
       " ('-pron-', 'find'),\n",
       " ('computer', 'science'),\n",
       " ('even', 'though'),\n",
       " ('north', 'korea'),\n",
       " ('high', 'school'),\n",
       " ('-pron-', 'feel'),\n",
       " ('social', 'medium'),\n",
       " ('-pron-', 'know'),\n",
       " ('would', 'happen'),\n",
       " ('get', 'rid'),\n",
       " ('major', 'accomplishment'),\n",
       " ('look', 'like'),\n",
       " ('jee', 'mains'),\n",
       " ('pro', 'con'),\n",
       " ('-pron-', 'ever'),\n",
       " ('-pron-', 'take'),\n",
       " ('tell', '-pron-'),\n",
       " ('new', 'york'),\n",
       " ('-pron-', 'need'),\n",
       " ('feel', 'like'),\n",
       " ('would', 'win'),\n",
       " ('tv', 'show'),\n",
       " ('harry', 'potter'),\n",
       " ('real', 'estate'),\n",
       " ('ssc', 'cgl'),\n",
       " ('saudi', 'arabia'),\n",
       " ('good', '-pron-'),\n",
       " ('star', 'wars'),\n",
       " ('good', 'place'),\n",
       " ('mechanical', 'engineering')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bigram vocabulary\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = nltk.BigramCollocationFinder.from_words(full_text)\n",
    "# scored = finder.score_ngrams( bigram_measures.likelihood_ratio  )\n",
    "bigram_vocab = finder.nbest(bigram_measures.likelihood_ratio, 40)\n",
    "bigram_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('\\x02tñ\\x7f¼é\\x1aaùõ\\x8d¶rwìiìñó', '\\x10œø'), 22.78494649905416)\n",
      "(('\\x10œø', '\\x17'), 22.78494649905416)\n",
      "(('\\x17', 'y.¾ƒe'), 22.78494649905416)\n",
      "(('+5', '=3\\\\sqrt'), 22.78494649905416)\n",
      "(('+\\\\sum_', '\\\\theta=8'), 22.78494649905416)\n",
      "(('+z^2', 'dzdydx'), 22.78494649905416)\n",
      "((',13sin', '13sin'), 22.78494649905416)\n",
      "(('-.1', 'b=3.4'), 22.78494649905416)\n",
      "(('-1,6', '-1,0'), 22.78494649905416)\n",
      "(('-12288', '-61440'), 22.78494649905416)\n",
      "(('-16t^2', '14t+4'), 22.78494649905416)\n",
      "(('-2.50', '-2.75.both'), 22.78494649905416)\n",
      "(('-207.113', '0.00206x'), 22.78494649905416)\n",
      "(('-210j', 'k^-1'), 22.78494649905416)\n",
      "(('-250', 'azithromycin-500'), 22.78494649905416)\n",
      "(('-2x+y', 'dx=0'), 22.78494649905416)\n",
      "((\"-2y'-3y\", 'te^'), 22.78494649905416)\n",
      "(('-3009', '-416'), 22.78494649905416)\n",
      "(('-4.4', '-4.04'), 22.78494649905416)\n",
      "(('-43.309439', '-97.978697'), 22.78494649905416)\n",
      "(('-5x3', '-33x2=3x=18'), 22.78494649905416)\n",
      "(('-6,8', '1,3,7'), 22.78494649905416)\n",
      "(('-61440', '-7290'), 22.78494649905416)\n",
      "(('-65610', '-12288'), 22.78494649905416)\n",
      "(('-68', 'bengali-65'), 22.78494649905416)\n",
      "(('-6x', 'ln|1'), 22.78494649905416)\n",
      "(('-6xy+29y^2', '6x-58y-151'), 22.78494649905416)\n",
      "(('-790', 'conker'), 22.78494649905416)\n",
      "(('-800', 'chem-790'), 22.78494649905416)\n",
      "(('-98304', '491520'), 22.78494649905416)\n"
     ]
    }
   ],
   "source": [
    "# create bigram vocabulary\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "\n",
    "\n",
    "finder3 = nltk.BigramCollocationFinder.from_words(full_text)\n",
    "finder3.apply_word_filter(lambda x: x in stopwords)\n",
    "scored = finder3.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n",
    "\n",
    "\n",
    "\n",
    "# finder = nltk.BigramCollocationFinder.from_words(full_text)\n",
    "# # scored = finder.score_ngrams( bigram_measures.likelihood_ratio  )\n",
    "# bigram_vocab = finder.nbest(bigram_measures.pmi, 40)\n",
    "# bigram_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-pron-', '-pron-', '-pron-'),\n",
       " ('-pron-', '-pron-', 'get'),\n",
       " ('-pron-', '-pron-', 'think'),\n",
       " ('would', '-pron-', '-pron-'),\n",
       " ('-pron-', '-pron-', 'want'),\n",
       " ('would', '-pron-', 'get'),\n",
       " ('-pron-', '-pron-', 'find'),\n",
       " ('-pron-', '-pron-', 'possible'),\n",
       " ('-pron-', '-pron-', 'feel'),\n",
       " ('-pron-', '-pron-', 'know'),\n",
       " ('tell', '-pron-', '-pron-'),\n",
       " ('-pron-', 'get', '-pron-'),\n",
       " ('-pron-', '-pron-', 'ever'),\n",
       " ('-pron-', '-pron-', 'take'),\n",
       " ('good', '-pron-', '-pron-'),\n",
       " ('-pron-', '-pron-', 'need'),\n",
       " ('-pron-', 'get', 'rid'),\n",
       " ('-pron-', '-pron-', 'see'),\n",
       " ('-pron-', '-pron-', 'favorite'),\n",
       " ('-pron-', '-pron-', 'true')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create trigram vocabulary\n",
    "trigram_measures = collocations.TrigramAssocMeasures()\n",
    "finder = nltk.TrigramCollocationFinder.from_words(full_text)\n",
    "trigram_vocab = finder.nbest(trigram_measures.likelihood_ratio, 20)\n",
    "trigram_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate text using ngrams\n",
    "def ngram_to_corpus(df, text_col, ngram_list, n, new_col):\n",
    "#     ngram_list = set({('let', 'us'), ('as', 'soon')})  # {('let', 'us'), ('as', 'soon')}\n",
    "#     tokens = ['please', 'let', 'us', 'know', 'as', 'soon', 'as', 'possible']\n",
    "    new_data = []\n",
    "    for text in df[text_col]:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        output = []\n",
    "        q_iter = iter(range(len(tokens)))\n",
    "        \n",
    "        for idx in q_iter:\n",
    "            output.append(tokens[idx])\n",
    "            if n == 2:\n",
    "                if idx < (len(tokens) - 1) and (tokens[idx], tokens[idx+1]) in ngram_list:\n",
    "                    output[-1] += '_' + tokens[idx+1]\n",
    "                    next(q_iter)\n",
    "            elif n == 3:\n",
    "                if idx < (len(tokens) - 2) and (tokens[idx], tokens[idx+1], tokens[idx+2] ) in ngram_list:\n",
    "                    output[-1] += '_' + tokens[idx+1] + '_' + tokens[idx+2]\n",
    "                    next(q_iter)\n",
    "                    next(q_iter)\n",
    "        new_data.append( ' '.join(output))\n",
    "    df[new_col] = new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text with bigram replacement\n",
    "ngram_to_corpus(train, 'question_text', bigram_vocab, 2, 'bigram_question_lkhd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text with both tri and bigram in text, by applying trigram first\n",
    "ngram_to_corpus(train, 'question_text', trigram_vocab, 3, 'trigram_question_lkhd')\n",
    "ngram_to_corpus(train, 'question_text', bigram_vocab, 2,  'trigram_question_lkhd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['bigram_questions_lkhd','trigram_questions_lkhd']]\n",
    "y = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f, X_test_f, y_train, y_test = train_test_split(X, y, stratify=y, random_state=90, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283.33333333333337"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
