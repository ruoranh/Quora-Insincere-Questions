{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text manipulation\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import *\n",
    "import scipy\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import nltk.collocations as collocations\n",
    "from nltk.tag import tnt\n",
    "import spacy\n",
    "\n",
    "# modelling\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of insincere questions: 80810\n",
      "No. of sincere questions: 1225312\n",
      "% of insincere questions: 0.06187017751787352\n",
      "Null score: 0.9381298224821265\n"
     ]
    }
   ],
   "source": [
    "no_insincere = train[train['target']==1].target.count()\n",
    "no_sincere = train[train['target']==0].target.count()\n",
    "\n",
    "print('No. of insincere questions:', no_insincere)\n",
    "print('No. of sincere questions:', no_sincere)\n",
    "print('% of insincere questions:', train.target.mean())\n",
    "print('Null score:', 1- train.target.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions and pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_trans(vectorizer, X_train, X_test):\n",
    "    # can also take a transformer\n",
    "    vect = vectorizer\n",
    "    vect.fit(X_train)\n",
    "    return vect.transform(X_train), vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinominalNB function for printing scores and storing into df.\n",
    "def model_score(model, X_train, X_test, y_train, y_test, score_df, model_label):\n",
    "    estimator = model\n",
    "    estimator.fit(X_train, y_train)\n",
    "    test_score =  estimator.score(X_test, y_test)\n",
    "    f1 = f1_score(y_test, estimator.predict(X_test))\n",
    "    \n",
    "    print('Train Accuracy :', estimator.score(X_train, y_train))\n",
    "    print('Test Accuracy:', test_score)\n",
    "    print('Test F1 score:', f1)\n",
    "    score_df.loc[model_label, 'Test_Accuracy'] = test_score\n",
    "    score_df.loc[model_label, 'Test_F1_score'] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validate function for printing scores and storing into df.\n",
    "def cv_score(model, X, y, model_label,  cv=5, ):    \n",
    "    \n",
    "    # instantiating model\n",
    "    estimator = model\n",
    "    \n",
    "    cv_result = cross_validate(estimator, X, y, cv = cv, n_jobs=-1, scoring=['accuracy', 'f1'])\n",
    "    \n",
    "    print('Test Accuracy Mean:',cv_result['test_accuracy'].mean())\n",
    "    print('Test Accuracy STD:',cv_result['test_accuracy'].std())\n",
    "    print('Test F1:', cv_result['test_f1'].mean())\n",
    "    score_df.loc[model_label, 'CV_Accuracy'] = cv_result['test_accuracy'].mean()\n",
    "    score_df.loc[model_label, 'CV_Acc_STD'] = cv_result['test_accuracy'].std()\n",
    "    score_df.loc[model_label, 'CV_F1_score'] = cv_result['test_f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV function, auto display best score and parameters and storing in df\n",
    "def gridcv(model, X, y, params, cv= 5 ):\n",
    "    \n",
    "    # instantiating model can also be a pipeline\n",
    "    estimator = model\n",
    "    \n",
    "    gridcv = GridSearchCV(estimator=estimator, param_grid=params, cv = cv, verbose=10, n_jobs=6)\n",
    "    gridcv.fit(X, y)\n",
    "    \n",
    "    print(gridcv.best_params_)\n",
    "    print(gridcv.best_score_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2860a872b780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# CountVectorizer pipeline and parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m pipeCVNB = Pipeline([('CV',CountVectorizer(stop_words=stopwords)), \n\u001b[0m\u001b[0;32m      3\u001b[0m                     ('NB',MultinomialNB())])\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m paramsCVNB = {'CV__max_df':(1.0, 0.9, 0.8, 0.7),\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# CountVectorizer pipeline and parameters\n",
    "pipeCVNB = Pipeline([('CV',CountVectorizer(stop_words=stopwords)), \n",
    "                    ('NB',MultinomialNB())])\n",
    "\n",
    "paramsCVNB = {'CV__max_df':(1.0, 0.9, 0.8, 0.7),\n",
    "       'CV__min_df': (1, 2, 0.01 , 0.1, 0.2),\n",
    "         'CV__ngram_range':((1,1), (1,2), (1,3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f9ed994baee8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TfidfVectorizer pipeline and parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m pipeTVNB = Pipeline([('TV',TfidfVectorizer(stop_words=stopwords)), \n\u001b[0m\u001b[0;32m      3\u001b[0m                     ('NB',MultinomialNB())])\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m paramsTVNB = {'TV__max_df':(1.0, 0.9, 0.8, 0.7, 0.6),\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer pipeline and parameters\n",
    "pipeTVNB = Pipeline([('TV',TfidfVectorizer(stop_words=stopwords)), \n",
    "                    ('NB',MultinomialNB())])\n",
    "\n",
    "paramsTVNB = {'TV__max_df':(1.0, 0.9, 0.8, 0.7, 0.6),\n",
    "       'TV__min_df': (1, 2, 0.01, 0.05, 0.1),\n",
    "         'TV__ngram_range':((1,1), (1,2), (1,3), (2,2), (2,3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spaCy to lemmatize using POS tags in one step, with out converting between WordNet and Treebank tags, using NLTK\n",
    "spac = spacy.load('en', disable=['parser', 'ner'])\n",
    "def lemmatizer(text):\n",
    "    text = spac(text)\n",
    "    return ' '.join([token.lemma_ for token in text if token.lemma_ not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48min 33s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# lemma_q = [lemmatizer(q) for q in train.question_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quebec nationalist see -PRON- province nation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-PRON- adopt dog would -PRON- encourage people...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>velocity affect time velocity affect space geo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>otto von guericke use magdeburg hemisphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-PRON- convert montra helicon mountain bike ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gaza slowly become auschwitz dachau treblinka ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quora automatically ban conservative opinion r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-PRON- crazy -PRON- wash wipe -PRON- grocery g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>thing dress moderately different dress modestly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-PRON- -PRON- -PRON- ever phase wherein -PRON-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-PRON- say feminism</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>calgary flames found</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dumb yet possibly true explanation trump elect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-PRON- use -PRON- external hard disk os well d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-PRON- 30 live home boyfriend -PRON- would lov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-PRON- know bram fischer rivonia trial</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>difficult -PRON- find good instructor take cla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-PRON- lick skin corpse</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-PRON- think amazon adopt house approach manuf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>many barony may exist within county palatine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-PRON- know whether girl sex sex -PRON-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-PRON- become fast learner -PRON- professional...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>united states become large dictatorship world</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>strange phenomenon -PRON- know witness generat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-PRON- leave -PRON- friend find new one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-PRON- make amazon alexa trigger event browser</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>two democracy never ever go full fledged war s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-PRON- top cbse 6 month</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-PRON- know visit mcleodganj triund trek</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>modern military submarine reduce noise achieve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306092</th>\n",
       "      <td>-PRON- hardly talk -PRON- interest reading his...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306093</th>\n",
       "      <td>-PRON- intimate relation -PRON- cousin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306094</th>\n",
       "      <td>-PRON- singer lyric voice -PRON- head religiou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306095</th>\n",
       "      <td>ginger plant naturally contain sugar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306096</th>\n",
       "      <td>technological advance medicine harm good humanity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306097</th>\n",
       "      <td>-PRON- pass class 11 math -PRON- 85 mark 100 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306098</th>\n",
       "      <td>-PRON- think physical trait -PRON- bear affect...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306099</th>\n",
       "      <td>pakis smell curry shit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306100</th>\n",
       "      <td>quora -PRON- good downvot answer -PRON- vote -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306101</th>\n",
       "      <td>wahabis muslim puritan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306102</th>\n",
       "      <td>step -PRON- take live normal life -PRON- suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306103</th>\n",
       "      <td>trump right usa benevolent towards -PRON- neig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306104</th>\n",
       "      <td>33 late career creative advertising</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306105</th>\n",
       "      <td>difference filteration work -PRON- liver filte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306106</th>\n",
       "      <td>universe pop existence nothing stop -PRON- pop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306107</th>\n",
       "      <td>share service technology team measure -PRON- v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306108</th>\n",
       "      <td>dsatm civil engineering</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306109</th>\n",
       "      <td>-PRON- know problem depend solely boundary geo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306110</th>\n",
       "      <td>comic idea -PRON- tube video shoot alone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306111</th>\n",
       "      <td>-PRON- 10 million bitcoin could -PRON- sell -P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306112</th>\n",
       "      <td>-PRON- ashamed indian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306113</th>\n",
       "      <td>method determine fossil age 10th std</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306114</th>\n",
       "      <td>-PRON- story today</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306115</th>\n",
       "      <td>-PRON- consume 150 gms protein daily vegetaria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306116</th>\n",
       "      <td>good career option msc chemistry student quali...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306117</th>\n",
       "      <td>technical skill -PRON- need computer science u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306118</th>\n",
       "      <td>ms ece good job prospect usa like india job pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306119</th>\n",
       "      <td>foam insulation toxic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306120</th>\n",
       "      <td>one start research project base biochemistry u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306121</th>\n",
       "      <td>win battle wolverine puma</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1306122 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question_text  target\n",
       "0        quebec nationalist see -PRON- province nation ...       0\n",
       "1        -PRON- adopt dog would -PRON- encourage people...       0\n",
       "2        velocity affect time velocity affect space geo...       0\n",
       "3               otto von guericke use magdeburg hemisphere       0\n",
       "4        -PRON- convert montra helicon mountain bike ch...       0\n",
       "5        gaza slowly become auschwitz dachau treblinka ...       0\n",
       "6        quora automatically ban conservative opinion r...       0\n",
       "7        -PRON- crazy -PRON- wash wipe -PRON- grocery g...       0\n",
       "8          thing dress moderately different dress modestly       0\n",
       "9        -PRON- -PRON- -PRON- ever phase wherein -PRON-...       0\n",
       "10                                     -PRON- say feminism       0\n",
       "11                                    calgary flames found       0\n",
       "12          dumb yet possibly true explanation trump elect       0\n",
       "13       -PRON- use -PRON- external hard disk os well d...       0\n",
       "14       -PRON- 30 live home boyfriend -PRON- would lov...       0\n",
       "15                  -PRON- know bram fischer rivonia trial       0\n",
       "16       difficult -PRON- find good instructor take cla...       0\n",
       "17                                 -PRON- lick skin corpse       0\n",
       "18       -PRON- think amazon adopt house approach manuf...       0\n",
       "19            many barony may exist within county palatine       0\n",
       "20                 -PRON- know whether girl sex sex -PRON-       0\n",
       "21       -PRON- become fast learner -PRON- professional...       0\n",
       "22           united states become large dictatorship world       1\n",
       "23       strange phenomenon -PRON- know witness generat...       0\n",
       "24                 -PRON- leave -PRON- friend find new one       0\n",
       "25          -PRON- make amazon alexa trigger event browser       0\n",
       "26       two democracy never ever go full fledged war s...       0\n",
       "27                                 -PRON- top cbse 6 month       0\n",
       "28                -PRON- know visit mcleodganj triund trek       0\n",
       "29       modern military submarine reduce noise achieve...       0\n",
       "...                                                    ...     ...\n",
       "1306092  -PRON- hardly talk -PRON- interest reading his...       0\n",
       "1306093             -PRON- intimate relation -PRON- cousin       1\n",
       "1306094  -PRON- singer lyric voice -PRON- head religiou...       1\n",
       "1306095               ginger plant naturally contain sugar       0\n",
       "1306096  technological advance medicine harm good humanity       0\n",
       "1306097  -PRON- pass class 11 math -PRON- 85 mark 100 1...       0\n",
       "1306098  -PRON- think physical trait -PRON- bear affect...       0\n",
       "1306099                             pakis smell curry shit       1\n",
       "1306100  quora -PRON- good downvot answer -PRON- vote -...       0\n",
       "1306101                             wahabis muslim puritan       0\n",
       "1306102  step -PRON- take live normal life -PRON- suffe...       0\n",
       "1306103  trump right usa benevolent towards -PRON- neig...       1\n",
       "1306104                33 late career creative advertising       0\n",
       "1306105  difference filteration work -PRON- liver filte...       0\n",
       "1306106  universe pop existence nothing stop -PRON- pop...       0\n",
       "1306107  share service technology team measure -PRON- v...       0\n",
       "1306108                            dsatm civil engineering       0\n",
       "1306109  -PRON- know problem depend solely boundary geo...       0\n",
       "1306110           comic idea -PRON- tube video shoot alone       0\n",
       "1306111  -PRON- 10 million bitcoin could -PRON- sell -P...       0\n",
       "1306112                              -PRON- ashamed indian       1\n",
       "1306113               method determine fossil age 10th std       0\n",
       "1306114                                 -PRON- story today       0\n",
       "1306115  -PRON- consume 150 gms protein daily vegetaria...       0\n",
       "1306116  good career option msc chemistry student quali...       0\n",
       "1306117  technical skill -PRON- need computer science u...       0\n",
       "1306118  ms ece good job prospect usa like india job pr...       0\n",
       "1306119                              foam insulation toxic       0\n",
       "1306120  one start research project base biochemistry u...       0\n",
       "1306121                          win battle wolverine puma       0\n",
       "\n",
       "[1306122 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemma_train = pd.DataFrame(lemma_q, columns = ['question_text'])\n",
    "# lemma_train['target'] = train.target\n",
    "# lemma_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemma_train.to_pickle('./lemma_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_train = pd.read_pickle('./lemma_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove digits\n",
    "clean_questions = [''.join(c for c in q if not c.isdigit()) for q in lemma_train.question_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(nltk.corpus.stopwords.words('english')) + list(string.punctuation) + [\"''\", '``','’','“','”', \"'s\", \"'d\", \"'ll\", \"'t\", \"n't\", \"ca\", 'wo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove stop words and lower all characters\n",
    "clean_questions = [' '.join(w for w in nltk.word_tokenize(q.lower()) if w not in stopwords) for q in clean_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all identity labels each question with a common labels\n",
    "def labels_to_question(data, label_list, label_type):\n",
    "         \n",
    "    new_data = []\n",
    "    \n",
    "    # For every questions\n",
    "    i_data = 0\n",
    "    for i_data in range(len(data)):\n",
    "        question = data[i_data].lower()\n",
    "        output = []\n",
    "        \n",
    "        # compare each label to question\n",
    "        for label in label_list:\n",
    "\n",
    "            if label in question:\n",
    "\n",
    "                que_t = nltk.word_tokenize(question)\n",
    "                lab_t = nltk.word_tokenize(label)\n",
    "\n",
    "                i_que = 0\n",
    "                while i_que < len(que_t):\n",
    "                    i_lab = 0\n",
    "                    \n",
    "                    # If current token is same as first label token, continue compare rest of the tokens. \n",
    "                    if que_t[i_que] == lab_t[0]:\n",
    "                        que_t[i_que] = label_type\n",
    "                        i_lab += 1\n",
    "                        i_que += 1\n",
    "\n",
    "                        # Remove trailing question tokens if they match trailing label tokens\n",
    "                        while i_lab < len(lab_t):\n",
    "                            if que_t[i_que] == lab_t[i_lab]:\n",
    "                                que_t.pop(i_que)\n",
    "                                i_lab += 1\n",
    "                            else:\n",
    "                                break\n",
    "#                     elif que_t[i_que] == lab_t[0]:\n",
    "#                         print('Question: ',question, i_data)\n",
    "#                         print('label: ', label)\n",
    "                    i_que += 1\n",
    "                question = ' '.join(que_t)\n",
    "#                 print('after: ', question)\n",
    "#                 print('label: ', label)\n",
    "        new_data.append(question)                   \n",
    "        if i_data % 1000 == 0:\n",
    "            clear_output(wait=True)\n",
    "            display(i_data)\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('nationalities.txt', 'r')\n",
    "nationalities = []\n",
    "for n in f:\n",
    "    nationalities.append(n.strip().lower())\n",
    "f.close()\n",
    "nationalities = set(nationalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity groupd filters, created from online lists, most frequent insincere words and manual editing.\n",
    "ID_filter = pd.read_csv('ID_filter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RELIGIOUS_ID</th>\n",
       "      <th>RACIAL_ID</th>\n",
       "      <th>NATIONAL_ID</th>\n",
       "      <th>NATIONALITY_ID</th>\n",
       "      <th>GENDER_ID</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Political_groups</th>\n",
       "      <th>Political_figure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buddhist</td>\n",
       "      <td>white people</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Afghans</td>\n",
       "      <td>girls</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trump supporters</td>\n",
       "      <td>donald trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>catholic</td>\n",
       "      <td>black people</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Albanians</td>\n",
       "      <td>boys</td>\n",
       "      <td>NaN</td>\n",
       "      <td>democrate</td>\n",
       "      <td>president trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RELIGIOUS_ID     RACIAL_ID  NATIONAL_ID NATIONALITY_ID GENDER_ID  \\\n",
       "0     buddhist  white people  Afghanistan        Afghans     girls   \n",
       "1     catholic  black people      Albania      Albanians      boys   \n",
       "\n",
       "   Unnamed: 5  Political_groups Political_figure  \n",
       "0         NaN  trump supporters     donald trump  \n",
       "1         NaN         democrate  president trump  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_filter.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "religious_ID = ID_filter.RELIGIOUS_ID.dropna()\n",
    "racial_ID = ID_filter.RACIAL_ID.dropna()\n",
    "national_ID = ID_filter.NATIONAL_ID.dropna()\n",
    "nationality_ID = ID_filter.NATIONALITY_ID.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clear_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2430c98509b3>\u001b[0m in \u001b[0;36mlabels_to_question\u001b[1;34m(data, label_list, label_type)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi_data\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clear_output' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_questions = labels_to_question(clean_questions, religious_ID, 'RELIGIOUS_ID')\n",
    "clean_questions = labels_to_question(clean_questions, racial_ID, 'RACIAL_ID')\n",
    "clean_questions = labels_to_question(clean_questions, national_ID, 'NATIONAL_ID')\n",
    "clean_questions = labels_to_question(clean_questions, nationality_ID, 'NATIONALITY_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_questions\n",
    "y = lemma_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state = 495)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default count vectorizer on raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(train.question_text, train.target,\n",
    "                                                                    stratify=train.target, random_state = 495)\n",
    "\n",
    "X_train_raw_t, X_test_raw_t=  vect_trans(CountVectorizer(), X_train_raw, X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9350739237089765\n",
      "Test Accuracy: 0.9344135778838762\n",
      "Test F1 score: 0.5646092542896641\n",
      "Test Accuracy Mean: 0.9321614838567932\n",
      "Test Accuracy STD: 0.0005228593021778298\n",
      "Test F1: 0.5489215714930169\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.934414</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.548922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB       0.934414       0.564609     0.932161    0.000523   \n",
       "\n",
       "              CV_F1_score  \n",
       "Raw_token_NB     0.548922  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Raw_token_NB'\n",
    "\n",
    "model_score(model, X_train_raw_t, X_test_raw_t, y_train_raw, y_test_raw, score_df, model_label)\n",
    "cv_score(model, X_train_raw_t, y_train_raw, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9350739237089765\n",
      "test score: 0.9344135778838762\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_raw_t, y_train_raw)  \n",
    "test_score =  nb.score(X_test_raw_t, y_test_raw)\n",
    "print('train score:', nb.score(X_train_raw_t, y_train_raw))\n",
    "print('test score:', test_score)\n",
    "y_pred = nb.predict(X_test_raw_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5646092542896641\n",
      "0.7645724512273393\n",
      "0.9344135778838762\n",
      "0.9397915566837656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[291229,  15099],\n",
       "       [  6317,  13886]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f1_score(y_test_raw, y_pred) )\n",
    "print(f1_score(y_test_raw, y_pred, average='macro') )\n",
    "print(f1_score(y_test_raw, y_pred, average='micro') )\n",
    "print(f1_score(y_test_raw, y_pred, average='weighted') )\n",
    "confusion_matrix(y_test_raw, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Token and Ngram modelling on cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_questions\n",
    "y = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state = 495)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_t, X_test_t=  vect_trans(CountVectorizer(max_df=1.0, min_df=1, ngram_range=(1,1)), X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9363397581235434\n",
      "Test Accuracy: 0.9363429505927462\n",
      "Test F1 score: 0.5443665059184568\n",
      "Test Accuracy Mean: 0.9314601699428223\n",
      "Test Accuracy STD: 0.000542872136791639\n",
      "Test F1: 0.516455592315815\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.934414</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.548922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936343</td>\n",
       "      <td>0.544367</td>\n",
       "      <td>0.931460</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.516456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB       0.934414       0.564609     0.932161    0.000523   \n",
       "Token_NB           0.936343       0.544367     0.931460    0.000543   \n",
       "\n",
       "              CV_F1_score  \n",
       "Raw_token_NB     0.548922  \n",
       "Token_NB         0.516456  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Token_NB'\n",
    "X_train_arg = X_train_t\n",
    "X_test_arg = X_test_t\n",
    "\n",
    "model_score(model, X_train_arg, X_test_arg, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_arg, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_bi, X_test_bi=  vect_trans(CountVectorizer(ngram_range=(1,2)), X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9642411986226905\n",
      "Test Accuracy: 0.9461031265025373\n",
      "Test F1 score: 0.4236072446205745\n",
      "Test Accuracy Mean: 0.9378138426556836\n",
      "Test Accuracy STD: 0.00017382141980152566\n",
      "Test F1: 0.4507294031911434\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.934414</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.548922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936343</td>\n",
       "      <td>0.544367</td>\n",
       "      <td>0.931460</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.516456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_NB</th>\n",
       "      <td>0.946103</td>\n",
       "      <td>0.423607</td>\n",
       "      <td>0.937814</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.450729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB       0.934414       0.564609     0.932161    0.000523   \n",
       "Token_NB           0.936343       0.544367     0.931460    0.000543   \n",
       "Bigram_NB          0.946103       0.423607     0.937814    0.000174   \n",
       "\n",
       "              CV_F1_score  \n",
       "Raw_token_NB     0.548922  \n",
       "Token_NB         0.516456  \n",
       "Bigram_NB        0.450729  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Bigram_NB'\n",
    "X_train_arg = X_train_bi\n",
    "X_test_arg = X_test_bi\n",
    "\n",
    "model_score(model, X_train_arg, X_test_arg, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_arg, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = RandomForestClassifier(n_estimators= 100, max_depth=20, n_jobs=-1)\n",
    "# model_label = 'Bigram_RF'\n",
    "\n",
    "\n",
    "# model_score(model, X_train_bi, X_test_bi, y_train, y_test, score_df, model_label)\n",
    "# cv_score(model, X_train_bi, y_train, model_label)\n",
    "# score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_tri, X_test_tri=  vect_trans(CountVectorizer(ngram_range=(1,3), stop_words=stopwords), X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9808746711637816\n",
      "Test Accuracy: 0.9442441912100229\n",
      "Test F1 score: 0.28170125463584\n",
      "Test Accuracy Mean: 0.9393644896978772\n",
      "Test Accuracy STD: 0.0003049995625848121\n",
      "Test F1: 0.46121678736240074\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.934414</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.548922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936343</td>\n",
       "      <td>0.544367</td>\n",
       "      <td>0.931460</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.516456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_NB</th>\n",
       "      <td>0.946103</td>\n",
       "      <td>0.423607</td>\n",
       "      <td>0.937814</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.450729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_NB</th>\n",
       "      <td>0.944244</td>\n",
       "      <td>0.281701</td>\n",
       "      <td>0.939364</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.461217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB       0.934414       0.564609     0.932161    0.000523   \n",
       "Token_NB           0.936343       0.544367     0.931460    0.000543   \n",
       "Bigram_NB          0.946103       0.423607     0.937814    0.000174   \n",
       "Trigram_NB         0.944244       0.281701     0.939364    0.000305   \n",
       "\n",
       "              CV_F1_score  \n",
       "Raw_token_NB     0.548922  \n",
       "Token_NB         0.516456  \n",
       "Bigram_NB        0.450729  \n",
       "Trigram_NB       0.461217  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Trigram_NB'\n",
    "X_train_arg = X_train_tri\n",
    "X_test_arg = X_test_tri\n",
    "\n",
    "model_score(model, X_train_arg, X_test_arg, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_arg, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = RandomForestClassifier(n_estimators= 100, max_depth=20, n_jobs=-1)\n",
    "# model_label = 'Trigram_RF'\n",
    "\n",
    "# model_score(model, X_train_tri, X_test_tri, y_train, y_test, score_df, model_label)\n",
    "# cv_score(model, X_train_tri, y_train, model_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch min/max df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'CV__max_df':(1.0, 0.9),\n",
    "#        'CV__min_df': (1, 2, 0.01, 0.02),\n",
    "#         'CV__ngram_range':((1,1), (1,2), (1,3))}\n",
    "\n",
    "# gridcv(pipeCVNB, X_train, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# X_train_t, X_test_t=  vect_trans(CountVectorizer(max_df=1.0, min_df=1, ngram_range=(1,2), \n",
    "#                                                     stop_words=stopwords), X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MultinomialNB()\n",
    "# model_label = 'Grid_DFNB'\n",
    "\n",
    "# model_score(model, X_train_t, X_test_t, y_train, y_test, score_df, model_label)\n",
    "# cv_score(model, X_train_t, y_train, model_label)\n",
    "# score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf_t, X_test_tf_t = vect_trans(TfidfTransformer(), X_train_t, X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9422473256695907\n",
      "Test Accuracy: 0.9411357574012881\n",
      "Test F1 score: 0.13007467752885268\n",
      "Test Accuracy Mean: 0.9398626572621417\n",
      "Test Accuracy STD: 9.97902774433833e-05\n",
      "Test F1: 0.09848172230931387\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.934414</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.548922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936343</td>\n",
       "      <td>0.544367</td>\n",
       "      <td>0.931460</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.516456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_NB</th>\n",
       "      <td>0.946103</td>\n",
       "      <td>0.423607</td>\n",
       "      <td>0.937814</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.450729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_NB</th>\n",
       "      <td>0.944244</td>\n",
       "      <td>0.281701</td>\n",
       "      <td>0.939364</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.461217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf_t_NB</th>\n",
       "      <td>0.941136</td>\n",
       "      <td>0.130075</td>\n",
       "      <td>0.939863</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.098482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB       0.934414       0.564609     0.932161    0.000523   \n",
       "Token_NB           0.936343       0.544367     0.931460    0.000543   \n",
       "Bigram_NB          0.946103       0.423607     0.937814    0.000174   \n",
       "Trigram_NB         0.944244       0.281701     0.939364    0.000305   \n",
       "Tfidf_t_NB         0.941136       0.130075     0.939863    0.000100   \n",
       "\n",
       "              CV_F1_score  \n",
       "Raw_token_NB     0.548922  \n",
       "Token_NB         0.516456  \n",
       "Bigram_NB        0.450729  \n",
       "Trigram_NB       0.461217  \n",
       "Tfidf_t_NB       0.098482  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Tfidf_t_NB'\n",
    "\n",
    "model_score(model, X_train_tf_t , X_test_tf_t, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_tf_t, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9422473256695907\n",
      "test score: 0.9411357574012881\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tf_t, y_train)  \n",
    "test_score =  nb.score(X_test_tf_t, y_test)\n",
    "print('train score:', nb.score(X_train_tf_t, y_train))\n",
    "print('test score:', test_score)\n",
    "y_pred = nb.predict(X_test_tf_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13007467752885268\n",
      "0.5498059558236386\n",
      "0.9411357574012881\n",
      "0.9175983308266111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[305873,    455],\n",
       "       [ 18766,   1437]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f1_score(y_test, y_pred) )\n",
    "print(f1_score(y_test, y_pred, average='macro') )\n",
    "print(f1_score(y_test, y_pred, average='micro') )\n",
    "print(f1_score(y_test, y_pred, average='weighted') )\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Best Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate text using ngrams\n",
    "def ngram_to_corpus(data, ngram_list, n):\n",
    "#     ngram_list = set({('let', 'us'), ('as', 'soon')})  # {('let', 'us'), ('as', 'soon')}\n",
    "#     tokens = ['please', 'let', 'us', 'know', 'as', 'soon', 'as', 'possible']\n",
    "    new_data = []\n",
    "    for text in data:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        output = []\n",
    "        q_iter = iter(range(len(tokens)))\n",
    "        \n",
    "        for idx in q_iter:\n",
    "            output.append(tokens[idx])\n",
    "            if n == 2:\n",
    "                if idx < (len(tokens) - 1) and (tokens[idx], tokens[idx+1]) in ngram_list:\n",
    "                    output[-1] += '_' + tokens[idx+1]\n",
    "                    next(q_iter)\n",
    "            elif n == 3:\n",
    "                if idx < (len(tokens) - 2) and (tokens[idx], tokens[idx+1], tokens[idx+2] ) in ngram_list:\n",
    "                    output[-1] += '_' + tokens[idx+1] + '_' + tokens[idx+2]\n",
    "                    next(q_iter)\n",
    "                    next(q_iter)\n",
    "        new_data.append( ' '.join(output))\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create one list of all question tokens\n",
    "full_text = []\n",
    "\n",
    "for text in X_train:\n",
    "    full_text += [w for w in nltk.word_tokenize(text) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7101029"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'would' in stopwords:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-pron-', '-pron-'), ('-pron-', 'get'), ('united', 'states'), ('year', 'old'), ('good', 'way'), ('donald', 'trump'), ('-pron-', 'think'), ('would', '-pron-'), ('-pron-', 'want'), ('-pron-', 'possible'), ('computer', 'science'), ('-pron-', 'find'), ('even', 'though'), ('north', 'korea'), ('high', 'school'), ('social', 'medium'), ('-pron-', 'feel'), ('-pron-', 'know'), ('would', 'happen'), ('get', 'rid'), ('major', 'accomplishment'), ('jee', 'mains'), ('look', 'like'), ('pro', 'con'), ('-pron-', 'ever'), ('-pron-', 'take'), ('tell', '-pron-'), ('new', 'york'), ('-pron-', 'need'), ('feel', 'like'), ('would', 'win'), ('tv', 'show'), ('harry', 'potter'), ('real', 'estate'), ('ssc', 'cgl'), ('saudi', 'arabia'), ('good', '-pron-'), ('star', 'wars'), ('mechanical', 'engineering'), ('good', 'place'), ('programming', 'language'), ('elon', 'musk'), ('hillary', 'clinton'), ('credit', 'card'), ('-pron-', 'favorite'), ('hong', 'kong'), ('mutual', 'fund'), ('tamil', 'nadu'), ('-pron-', 'true'), ('video', 'game'), ('-pron-', 'see'), ('president', 'trump'), ('useful', 'tip'), ('prime', 'minister'), ('san', 'francisco'), ('artificial', 'intelligence'), ('long', 'term'), ('-pron-', 'make'), ('tv', 'series'), ('get', 'job'), ('narendra', 'modi'), ('year', 'ago'), ('many', 'people'), ('mental', 'illness'), ('black', 'hole'), ('new', 'zealand'), ('youtube', 'channel'), ('digital', 'marketing'), ('earn', 'money'), ('short', 'term'), ('literary', 'device'), ('los', 'angeles'), ('machine', 'learning'), ('-year', 'old'), ('someone', 'else'), ('middle', 'east'), ('stock', 'market'), ('software', 'engineer'), ('entrance', 'exam'), ('vice', 'versa')]\n",
      "Wall time: 38.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create bigram vocabulary\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = nltk.BigramCollocationFinder.from_words(full_text)\n",
    "# scored = finder.score_ngrams( bigram_measures.likelihood_ratio  )\n",
    "bigram_vocab = finder.nbest(bigram_measures.likelihood_ratio, 80)\n",
    "print(bigram_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pepto', 'bismol'), ('muhoozi', 'kainerugaba'), ('avada', 'kedavra'), ('michio', 'kaku'), ('neman', 'ashraf'), ('roald', 'dahl'), ('aam', 'aadmi'), ('buenos', 'aires'), ('jaggi', 'vasudev'), ('disha', 'patani'), ('ronda', 'rousey'), ('deng', 'xiaoping'), ('abercrombie', 'fitch'), ('zaira', 'wasim'), ('endoplasmic', 'reticulum'), ('nathuram', 'godse'), ('sushma', 'swaraj'), ('jiang', 'zemin'), ('vande', 'mataram'), ('pakatan', 'harapan'), ('asim', 'qureshi'), ('lata', 'mangeshkar'), ('sylvia', 'plath'), ('kalpit', 'veerwal'), ('sindhu', 'satish'), ('meryl', 'streep'), ('looney', 'tunes'), ('pradhan', 'mantri'), ('aldous', 'huxley'), ('dima', 'vorobiev'), ('ulcerative', 'colitis'), ('narsee', 'monjee'), ('gauri', 'lankesh'), ('mosin', 'nagant'), ('sonu', 'nigam'), ('jiu', 'jitsu'), ('shel', 'silverstein'), ('mitt', 'romney'), ('khaled', 'hosseini'), ('petyr', 'baelish'), ('sourav', 'ganguly'), ('satoshi', 'nakamoto'), ('ballon', \"d'or\"), ('satya', 'nadella'), ('agatha', 'christie'), ('tubal', 'ligation'), ('jules', 'verne'), ('jimi', 'hendrix'), ('raghuram', 'rajan'), ('lingua', 'franca'), ('yoko', 'ono'), ('nigel', 'farage'), ('hadron', 'collider'), ('smriti', 'irani'), ('klux', 'klan'), ('shweta', 'shalini'), ('scooby', 'doo'), ('forrest', 'gump'), ('jorah', 'mormont'), ('scarlett', 'johansson'), ('nicki', 'minaj'), ('hrithik', 'roshan'), ('stony', 'brook'), ('trinidad', 'tobago'), ('dushka', 'zapata'), ('nath', 'kovind'), ('magna', 'carta'), ('rudy', 'giuliani'), ('kendriya', 'vidyalaya'), ('barkha', 'dutt'), ('shinzo', 'abe'), ('chester', 'bennington'), ('falun', 'gong'), ('che', 'guevara'), ('dawood', 'ibrahim'), ('rabindranath', 'tagore'), ('vinay', 'kumaran'), ('arsene', 'wenger'), ('hafiz', 'saeed'), ('reza', 'pahlavi'), ('rolls', 'royce'), ('granth', 'sahib'), ('mein', 'kampf'), ('sergey', 'brin'), ('ravindrababu', 'ravula'), ('mustafa', 'kemal'), ('waldo', 'emerson'), ('ajit', 'pai'), ('kendrick', 'lamar'), ('notre', 'dame'), ('tsar', 'bomba'), ('elke', 'weiss'), ('deathly', 'hallows'), ('sandeep', 'maheshwari'), ('kellyanne', 'conway'), ('milo', 'yiannopoulos'), ('magnus', 'carlsen'), ('mace', 'windu'), ('kyrie', 'irving'), ('snoop', 'dogg'), ('spongebob', 'squarepants'), ('winnie', 'pooh'), ('lex', 'luthor'), ('hannibal', 'lecter'), ('nawaz', 'sharif'), ('shawshank', 'redemption'), ('zack', 'snyder'), ('boba', 'fett'), ('brock', 'lesnar'), ('krav', 'maga'), ('terence', 'tao'), ('lois', 'lowry'), ('zakir', 'naik'), ('noam', 'chomsky'), ('dalai', 'lama'), ('hans', 'zimmer'), ('hugh', 'hefner'), ('ku', 'klux'), ('tel', 'aviv'), ('sundar', 'pichai'), ('qui', 'gon'), ('kj', 'somaiya'), ('tic', 'tac'), ('berkshire', 'hathaway'), ('fullmetal', 'alchemist'), ('stephenie', 'meyer'), ('fidel', 'castro'), ('oprah', 'winfrey'), ('subramanian', 'swamy'), ('shashi', 'tharoor'), ('miley', 'cyrus'), ('burj', 'khalifa'), ('travis', 'kalanick'), ('chiang', 'mai'), ('muammar', 'gaddafi'), ('maxine', 'waters'), ('karni', 'sena'), ('paulo', 'coelho'), ('ralph', 'waldo'), ('kurt', 'cobain'), ('consiglio', 'devastations'), ('ku', 'leuven'), ('udita', 'pal'), ('ariana', 'grande'), ('parsvnath', 'developers'), ('otto', 'warmbier'), ('ivan', 'tregear'), ('acm', 'icpc'), ('dhinchak', 'pooja'), ('dungeons', 'dragons'), ('solitary', 'confinement'), ('bosnia', 'herzegovina'), ('mitch', 'mcconnell'), ('majin', 'buu'), ('mar', 'lago'), ('kulbhushan', 'jadhav'), ('brad', 'pitt'), ('andaman', 'nicobar'), ('kung', 'fu'), ('netaji', 'subhash'), ('selena', 'gomez'), ('aishwarya', 'rai'), ('habib', 'fanny'), ('dennis', 'rodman'), ('truman', 'capote'), ('deepika', 'padukone'), ('alkyl', 'halide'), ('jill', 'stein'), ('monte', 'carlo'), ('julian', 'assange'), ('tipu', 'sultan'), ('katrina', 'kaif'), ('martian', 'manhunter'), ('maya', 'angelou'), ('euron', 'greyjoy'), ('conor', 'mcgregor'), ('amitabh', 'bachchan'), ('nova', 'scotia'), ('arnab', 'goswami'), ('alia', 'bhatt'), ('planned', 'parenthood'), ('catcher', 'rye'), ('sergei', 'skripal'), ('arun', 'jaitley'), ('marilyn', 'monroe'), ('aesthetically', 'pleasing'), ('nikki', 'haley'), ('palo', 'alto'), ('sigmund', 'freud'), ('pawan', 'kalyan'), ('millia', 'islamia'), ('j.k.', 'rowling'), ('ayn', 'rand'), ('degrasse', 'tyson'), ('ping', 'pong'), ('alfie', 'evans'), ('são', 'paulo'), ('inguinal', 'hernia'), ('déjà', 'vu'), ('kemal', 'ataturk')]\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create bigram vocabulary\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "\n",
    "\n",
    "finder3 = nltk.BigramCollocationFinder.from_words(full_text)\n",
    "finder3.apply_freq_filter(10)\n",
    "finder3.apply_word_filter(lambda x: x in stopwords)\n",
    "best_pmi = finder3.nbest(bigram_measures.pmi, 200)\n",
    "print(best_pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-pron-', '-pron-', '-pron-'), ('-pron-', '-pron-', 'get'), ('-pron-', '-pron-', 'think'), ('would', '-pron-', '-pron-'), ('-pron-', '-pron-', 'want'), ('-pron-', '-pron-', 'find'), ('-pron-', '-pron-', 'possible'), ('would', '-pron-', 'get'), ('-pron-', '-pron-', 'feel'), ('-pron-', '-pron-', 'know'), ('tell', '-pron-', '-pron-'), ('good', '-pron-', '-pron-'), ('-pron-', '-pron-', 'ever'), ('-pron-', '-pron-', 'take'), ('-pron-', '-pron-', 'need'), ('-pron-', 'get', '-pron-'), ('-pron-', '-pron-', 'see'), ('-pron-', 'get', 'rid'), ('-pron-', '-pron-', 'favorite'), ('-pron-', '-pron-', 'true')]\n",
      "Wall time: 7min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create trigram vocabulary\n",
    "trigram_measures = collocations.TrigramAssocMeasures()\n",
    "finder = nltk.TrigramCollocationFinder.from_words(full_text)\n",
    "trigram_vocab = finder.nbest(trigram_measures.likelihood_ratio, 20)\n",
    "print(trigram_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create text with bigram replacement\n",
    "train['bigram_question_lkhd'] = ngram_to_corpus(clean_questions, bigram_vocab, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create text with both tri and bigram in text, by applying trigram first\n",
    "train['trigram_question_lkhd'] = ngram_to_corpus(clean_questions, trigram_vocab, 3)\n",
    "train['trigram_question_lkhd'] = ngram_to_corpus(train['trigram_question_lkhd'], bigram_vocab, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-pron-_know whether girl sex sex -pron-',\n",
       "       '-pron- become fast learner -pron- professional career -pron- personal life',\n",
       "       'united_states become large dictatorship world',\n",
       "       'strange phenomenon -pron-_know witness generate area electronic explanation term modern physics',\n",
       "       '-pron- leave -pron- friend find new one'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['trigram_question_lkhd'][20:25].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['bigram_question_lkhd','trigram_question_lkhd']]\n",
    "y = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=495, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1044897"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram_question_lkhd</th>\n",
       "      <th>trigram_question_lkhd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1077331</th>\n",
       "      <td>procedure officially change name india</td>\n",
       "      <td>procedure officially change name india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334276</th>\n",
       "      <td>ancient egypt polytheism</td>\n",
       "      <td>ancient egypt polytheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620299</th>\n",
       "      <td>whenever -pron- put blood pressure monitor -pr...</td>\n",
       "      <td>whenever -pron- put blood pressure monitor -pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098236</th>\n",
       "      <td>ego react suicide</td>\n",
       "      <td>ego react suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548923</th>\n",
       "      <td>-pron- join tcs fresher -pron- miss campus hiring</td>\n",
       "      <td>-pron- join tcs fresher -pron- miss campus hiring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      bigram_question_lkhd  \\\n",
       "1077331             procedure officially change name india   \n",
       "334276                            ancient egypt polytheism   \n",
       "620299   whenever -pron- put blood pressure monitor -pr...   \n",
       "1098236                                  ego react suicide   \n",
       "548923   -pron- join tcs fresher -pron- miss campus hiring   \n",
       "\n",
       "                                     trigram_question_lkhd  \n",
       "1077331             procedure officially change name india  \n",
       "334276                            ancient egypt polytheism  \n",
       "620299   whenever -pron- put blood pressure monitor -pr...  \n",
       "1098236                                  ego react suicide  \n",
       "548923   -pron- join tcs fresher -pron- miss campus hiring  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model using bigram text\n",
    "X_train_bi, X_test_bi =  vect_trans(CountVectorizer(max_df=1.0,  min_df=1, ngram_range=(1,1), stop_words=stopwords),\n",
    "                                   X_train.bigram_question_lkhd, X_test.bigram_question_lkhd,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.936412871316503\n",
      "Test Accuracy: 0.9361737965355537\n",
      "Test F1 score: 0.5445157765332604\n",
      "Test Accuracy Mean: 0.9316650352730174\n",
      "Test Accuracy STD: 0.0003564139612948782\n",
      "Test F1: 0.5184665706076068\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.934414</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.548922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936343</td>\n",
       "      <td>0.544367</td>\n",
       "      <td>0.931460</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.516456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_NB</th>\n",
       "      <td>0.946103</td>\n",
       "      <td>0.423607</td>\n",
       "      <td>0.937814</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.450729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_NB</th>\n",
       "      <td>0.944244</td>\n",
       "      <td>0.281701</td>\n",
       "      <td>0.939364</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.461217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf_t_NB</th>\n",
       "      <td>0.941136</td>\n",
       "      <td>0.130075</td>\n",
       "      <td>0.939863</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.098482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_best_NB</th>\n",
       "      <td>0.936174</td>\n",
       "      <td>0.544516</td>\n",
       "      <td>0.931665</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.518467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB         0.934414       0.564609     0.932161    0.000523   \n",
       "Token_NB             0.936343       0.544367     0.931460    0.000543   \n",
       "Bigram_NB            0.946103       0.423607     0.937814    0.000174   \n",
       "Trigram_NB           0.944244       0.281701     0.939364    0.000305   \n",
       "Tfidf_t_NB           0.941136       0.130075     0.939863    0.000100   \n",
       "Bigram_best_NB       0.936174       0.544516     0.931665    0.000356   \n",
       "\n",
       "                CV_F1_score  \n",
       "Raw_token_NB       0.548922  \n",
       "Token_NB           0.516456  \n",
       "Bigram_NB          0.450729  \n",
       "Trigram_NB         0.461217  \n",
       "Tfidf_t_NB         0.098482  \n",
       "Bigram_best_NB     0.518467  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Bigram_best_NB'\n",
    "\n",
    "model_score(model, X_train_bi, X_test_bi, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_bi, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model using bigram text\n",
    "X_train_tri, X_test_tri =  vect_trans(CountVectorizer(max_df=1.0,  min_df=1, ngram_range=(1,1), stop_words=stopwords),\n",
    "                                   X_train.trigram_question_lkhd, X_test.trigram_question_lkhd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9364157424128885\n",
      "Test Accuracy: 0.9361852808881233\n",
      "Test F1 score: 0.5446848027968972\n",
      "Test Accuracy Mean: 0.9316525939460216\n",
      "Test Accuracy STD: 0.0003618392672776735\n",
      "Test F1: 0.5184340670776231\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.934414</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.548922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936343</td>\n",
       "      <td>0.544367</td>\n",
       "      <td>0.931460</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.516456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_NB</th>\n",
       "      <td>0.946103</td>\n",
       "      <td>0.423607</td>\n",
       "      <td>0.937814</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.450729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_NB</th>\n",
       "      <td>0.944244</td>\n",
       "      <td>0.281701</td>\n",
       "      <td>0.939364</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.461217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf_t_NB</th>\n",
       "      <td>0.941136</td>\n",
       "      <td>0.130075</td>\n",
       "      <td>0.939863</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.098482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_best_NB</th>\n",
       "      <td>0.936174</td>\n",
       "      <td>0.544516</td>\n",
       "      <td>0.931665</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.518467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_best_NB</th>\n",
       "      <td>0.936185</td>\n",
       "      <td>0.544685</td>\n",
       "      <td>0.931653</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.518434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB          0.934414       0.564609     0.932161    0.000523   \n",
       "Token_NB              0.936343       0.544367     0.931460    0.000543   \n",
       "Bigram_NB             0.946103       0.423607     0.937814    0.000174   \n",
       "Trigram_NB            0.944244       0.281701     0.939364    0.000305   \n",
       "Tfidf_t_NB            0.941136       0.130075     0.939863    0.000100   \n",
       "Bigram_best_NB        0.936174       0.544516     0.931665    0.000356   \n",
       "Trigram_best_NB       0.936185       0.544685     0.931653    0.000362   \n",
       "\n",
       "                 CV_F1_score  \n",
       "Raw_token_NB        0.548922  \n",
       "Token_NB            0.516456  \n",
       "Bigram_NB           0.450729  \n",
       "Trigram_NB          0.461217  \n",
       "Tfidf_t_NB          0.098482  \n",
       "Bigram_best_NB      0.518467  \n",
       "Trigram_best_NB     0.518434  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Trigram_best_NB'\n",
    "\n",
    "model_score(model, X_train_tri, X_test_tri, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_tri, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regenerate bi/trigrams after replacing previous ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_text_ngrams = []\n",
    "\n",
    "for text in X_train.trigram_question_lkhd:\n",
    "    full_text_ngrams += [w for w in nltk.word_tokenize(text) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-pron-', '-pron-'), ('long', '-pron-_take'), ('good', '-pron-'), ('-pron-_feel', 'like'), ('-pron-', 'start'), ('-pron-', 'stop'), ('-pron-', 'mean'), ('south', 'korea'), ('-pron-', 'go'), ('would_win', 'fight'), ('good', 'book'), ('lose', 'weight'), ('trump', 'supporter'), ('-pron-', 'use'), ('civil', 'engineering'), ('kim', 'jong'), ('real', 'life'), ('advantage', 'disadvantage'), ('hotel', 'short_term'), ('fall', 'love'), ('-pron-', 'buy'), ('different', 'type'), ('useful_tip', 'someone'), ('police', 'officer'), ('mechanical', 'engineer'), ('commit', 'suicide'), ('international', 'student'), ('advice', 'would_-pron-'), ('much', 'money'), ('-pron-', 'get'), ('global', 'warming'), ('south', 'africa'), ('-pron-', 'prepare'), ('-pron-', 'life'), ('jong', 'un'), ('-pron-', 'opinion'), ('chance', 'get'), ('world', 'war'), ('personality', 'disorder'), ('silicon', 'valley')]\n",
      "Wall time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create bigram vocabulary\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = nltk.BigramCollocationFinder.from_words(full_text_ngrams)\n",
    "# scored = finder.score_ngrams( bigram_measures.likelihood_ratio  )\n",
    "bigram_vocab = finder.nbest(bigram_measures.likelihood_ratio, 40)\n",
    "print(bigram_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kim', 'jong', 'un'), ('borderline', 'personality', 'disorder'), ('controversial', 'event', 'mention'), ('short_term', 'business', 'traveler'), ('rbi', 'grade', 'b'), ('consideration', 'write', 'biography'), ('hotel', 'short_term', 'business'), ('fifa', 'world', 'cup'), ('writing', 'style', 'structure'), ('tip', 'write', 'summary'), ('manufacturing', 'process', 'improve'), ('-pron-_take', 'consideration', 'write'), ('character', 'change', 'throughout'), ('good', 'hotel', 'short_term'), ('world', 'war', 'ii'), ('download', 'test', 'bank'), ('useful_tip', 'someone', 'start'), ('-pron-', 'wan', 'na'), ('student', 'organization', 'join'), ('less', 'know', 'fact')]\n",
      "Wall time: 35.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create trigram vocabulary\n",
    "trigram_measures = collocations.TrigramAssocMeasures()\n",
    "finder = nltk.TrigramCollocationFinder.from_words(full_text_ngrams)\n",
    "finder.apply_freq_filter(100)\n",
    "trigram_vocab = finder.nbest(trigram_measures.pmi, 20)\n",
    "print(trigram_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create text with bigram replacement\n",
    "train['bigram_question_2'] = ngram_to_corpus(clean_questions, bigram_vocab, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create text with both tri and bigram in text, by applying trigram first\n",
    "train['trigram_question_2'] = ngram_to_corpus(clean_questions, trigram_vocab, 3)\n",
    "train['trigram_question_2'] = ngram_to_corpus(train['trigram_question_2'], bigram_vocab, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['bigram_question_2','trigram_question_2']]\n",
    "y = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=495, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1044897"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model using bigram text\n",
    "X_train_bi, X_test_bi =  vect_trans(CountVectorizer(max_df=1.0,  min_df=1, ngram_range=(1,1), stop_words=stopwords),\n",
    "                                   X_train.bigram_question_2, X_test.bigram_question_2,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9361803125092713\n",
      "Test Accuracy: 0.9359441094841612\n",
      "Test F1 score: 0.5431761718856644\n",
      "Test Accuracy Mean: 0.9314449178971914\n",
      "Test Accuracy STD: 0.00025738958358852273\n",
      "Test F1: 0.5185952596975845\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.934414</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.548922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936343</td>\n",
       "      <td>0.544367</td>\n",
       "      <td>0.931460</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.516456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_NB</th>\n",
       "      <td>0.946103</td>\n",
       "      <td>0.423607</td>\n",
       "      <td>0.937814</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.450729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_NB</th>\n",
       "      <td>0.944244</td>\n",
       "      <td>0.281701</td>\n",
       "      <td>0.939364</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.461217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf_t_NB</th>\n",
       "      <td>0.941136</td>\n",
       "      <td>0.130075</td>\n",
       "      <td>0.939863</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.098482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_best_NB</th>\n",
       "      <td>0.936174</td>\n",
       "      <td>0.544516</td>\n",
       "      <td>0.931665</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.518467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_best_NB</th>\n",
       "      <td>0.936185</td>\n",
       "      <td>0.544685</td>\n",
       "      <td>0.931653</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.518434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_best_NB2</th>\n",
       "      <td>0.935944</td>\n",
       "      <td>0.543176</td>\n",
       "      <td>0.931445</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.518595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB          0.934414       0.564609     0.932161    0.000523   \n",
       "Token_NB              0.936343       0.544367     0.931460    0.000543   \n",
       "Bigram_NB             0.946103       0.423607     0.937814    0.000174   \n",
       "Trigram_NB            0.944244       0.281701     0.939364    0.000305   \n",
       "Tfidf_t_NB            0.941136       0.130075     0.939863    0.000100   \n",
       "Bigram_best_NB        0.936174       0.544516     0.931665    0.000356   \n",
       "Trigram_best_NB       0.936185       0.544685     0.931653    0.000362   \n",
       "Bigram_best_NB2       0.935944       0.543176     0.931445    0.000257   \n",
       "\n",
       "                 CV_F1_score  \n",
       "Raw_token_NB        0.548922  \n",
       "Token_NB            0.516456  \n",
       "Bigram_NB           0.450729  \n",
       "Trigram_NB          0.461217  \n",
       "Tfidf_t_NB          0.098482  \n",
       "Bigram_best_NB      0.518467  \n",
       "Trigram_best_NB     0.518434  \n",
       "Bigram_best_NB2     0.518595  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Bigram_best_NB2'\n",
    "\n",
    "model_score(model, X_train_bi, X_test_bi, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_bi, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model using bigram text\n",
    "X_train_tri, X_test_tri =  vect_trans(CountVectorizer(max_df=1.0,  min_df=1, ngram_range=(1,1), stop_words=stopwords),\n",
    "                                   X_train.trigram_question_2, X_test.trigram_question_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9362128516016411\n",
      "Test Accuracy: 0.9359747344243469\n",
      "Test F1 score: 0.5432698872170185\n",
      "Test Accuracy Mean: 0.9314937264459875\n",
      "Test Accuracy STD: 0.00023703741512209992\n",
      "Test F1: 0.5186372571655607\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_score</th>\n",
       "      <th>CV_Accuracy</th>\n",
       "      <th>CV_Acc_STD</th>\n",
       "      <th>CV_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw_token_NB</th>\n",
       "      <td>0.934414</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.548922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token_NB</th>\n",
       "      <td>0.936343</td>\n",
       "      <td>0.544367</td>\n",
       "      <td>0.931460</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.516456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_NB</th>\n",
       "      <td>0.946103</td>\n",
       "      <td>0.423607</td>\n",
       "      <td>0.937814</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.450729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_NB</th>\n",
       "      <td>0.944244</td>\n",
       "      <td>0.281701</td>\n",
       "      <td>0.939364</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.461217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfidf_t_NB</th>\n",
       "      <td>0.941136</td>\n",
       "      <td>0.130075</td>\n",
       "      <td>0.939863</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.098482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_best_NB</th>\n",
       "      <td>0.936174</td>\n",
       "      <td>0.544516</td>\n",
       "      <td>0.931665</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.518467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_best_NB</th>\n",
       "      <td>0.936185</td>\n",
       "      <td>0.544685</td>\n",
       "      <td>0.931653</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.518434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram_best_NB2</th>\n",
       "      <td>0.935944</td>\n",
       "      <td>0.543176</td>\n",
       "      <td>0.931445</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.518595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram_best_NB2</th>\n",
       "      <td>0.935975</td>\n",
       "      <td>0.543270</td>\n",
       "      <td>0.931494</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.518637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Test_Accuracy  Test_F1_score  CV_Accuracy  CV_Acc_STD  \\\n",
       "Raw_token_NB           0.934414       0.564609     0.932161    0.000523   \n",
       "Token_NB               0.936343       0.544367     0.931460    0.000543   \n",
       "Bigram_NB              0.946103       0.423607     0.937814    0.000174   \n",
       "Trigram_NB             0.944244       0.281701     0.939364    0.000305   \n",
       "Tfidf_t_NB             0.941136       0.130075     0.939863    0.000100   \n",
       "Bigram_best_NB         0.936174       0.544516     0.931665    0.000356   \n",
       "Trigram_best_NB        0.936185       0.544685     0.931653    0.000362   \n",
       "Bigram_best_NB2        0.935944       0.543176     0.931445    0.000257   \n",
       "Trigram_best_NB2       0.935975       0.543270     0.931494    0.000237   \n",
       "\n",
       "                  CV_F1_score  \n",
       "Raw_token_NB         0.548922  \n",
       "Token_NB             0.516456  \n",
       "Bigram_NB            0.450729  \n",
       "Trigram_NB           0.461217  \n",
       "Tfidf_t_NB           0.098482  \n",
       "Bigram_best_NB       0.518467  \n",
       "Trigram_best_NB      0.518434  \n",
       "Bigram_best_NB2      0.518595  \n",
       "Trigram_best_NB2     0.518637  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model_label = 'Trigram_best_NB2'\n",
    "\n",
    "model_score(model, X_train_tri, X_test_tri, y_train, y_test, score_df, model_label)\n",
    "cv_score(model, X_train_tri, y_train, model_label)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
