{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import operator\n",
    "import langid\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1306122, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of insincere questions: 80810\n",
      "No. of sincere questions: 1225312\n",
      "% of insincere questions: 0.06187017751787352\n",
      "Null score: 0.9381298224821265\n"
     ]
    }
   ],
   "source": [
    "no_insincere = train[train['target']==1].target.count()\n",
    "no_sincere = train[train['target']==0].target.count()\n",
    "\n",
    "print('No. of insincere questions:', no_insincere)\n",
    "print('No. of sincere questions:', no_sincere)\n",
    "print('% of insincere questions:', train.target.mean())\n",
    "print('Null score:', 1- train.target.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Questions into single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full text in one corpus\n",
    "full_text = str()\n",
    "for s in train['question_text']:\n",
    "    full_text = full_text + s + ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full text into one list of single tokens, removing capitalisation and non-alpha values.\n",
    "# Worth reconsidering capitalisation could be useful in determining insincerity.\n",
    "\n",
    "full_text_list = [ w.lower() for s in train['question_text'] for w in nltk.word_tokenize(s) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', 'did'),\n",
       " ('did', 'quebec'),\n",
       " ('quebec', 'nationalists'),\n",
       " ('nationalists', 'see'),\n",
       " ('see', 'their'),\n",
       " ('their', 'province'),\n",
       " ('province', 'as'),\n",
       " ('as', 'a'),\n",
       " ('a', 'nation'),\n",
       " ('nation', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'do'),\n",
       " ('do', 'you'),\n",
       " ('you', 'have'),\n",
       " ('have', 'an'),\n",
       " ('an', 'adopted'),\n",
       " ('adopted', 'dog'),\n",
       " ('dog', 'how'),\n",
       " ('how', 'would'),\n",
       " ('would', 'you'),\n",
       " ('you', 'encourage'),\n",
       " ('encourage', 'people'),\n",
       " ('people', 'to'),\n",
       " ('to', 'adopt'),\n",
       " ('adopt', 'and'),\n",
       " ('and', 'not'),\n",
       " ('not', 'shop'),\n",
       " ('shop', 'why'),\n",
       " ('why', 'does'),\n",
       " ('does', 'velocity'),\n",
       " ('velocity', 'affect'),\n",
       " ('affect', 'time'),\n",
       " ('time', 'does'),\n",
       " ('does', 'velocity'),\n",
       " ('velocity', 'affect'),\n",
       " ('affect', 'space'),\n",
       " ('space', 'geometry'),\n",
       " ('geometry', 'how'),\n",
       " ('how', 'did'),\n",
       " ('did', 'otto'),\n",
       " ('otto', 'von'),\n",
       " ('von', 'guericke'),\n",
       " ('guericke', 'used'),\n",
       " ('used', 'the'),\n",
       " ('the', 'magdeburg'),\n",
       " ('magdeburg', 'hemispheres'),\n",
       " ('hemispheres', 'can'),\n",
       " ('can', 'i'),\n",
       " ('i', 'convert'),\n",
       " ('convert', 'montra'),\n",
       " ('montra', 'helicon'),\n",
       " ('helicon', 'd'),\n",
       " ('d', 'to'),\n",
       " ('to', 'a'),\n",
       " ('a', 'mountain'),\n",
       " ('mountain', 'bike'),\n",
       " ('bike', 'by'),\n",
       " ('by', 'just'),\n",
       " ('just', 'changing'),\n",
       " ('changing', 'the'),\n",
       " ('the', 'tyres'),\n",
       " ('tyres', 'is'),\n",
       " ('is', 'gaza'),\n",
       " ('gaza', 'slowly'),\n",
       " ('slowly', 'becoming'),\n",
       " ('becoming', 'auschwitz'),\n",
       " ('auschwitz', 'dachau'),\n",
       " ('dachau', 'or'),\n",
       " ('or', 'treblinka'),\n",
       " ('treblinka', 'for'),\n",
       " ('for', 'palestinians'),\n",
       " ('palestinians', 'why'),\n",
       " ('why', 'does'),\n",
       " ('does', 'quora'),\n",
       " ('quora', 'automatically'),\n",
       " ('automatically', 'ban'),\n",
       " ('ban', 'conservative'),\n",
       " ('conservative', 'opinions'),\n",
       " ('opinions', 'when'),\n",
       " ('when', 'reported'),\n",
       " ('reported', 'but'),\n",
       " ('but', 'does'),\n",
       " ('does', 'not'),\n",
       " ('not', 'do'),\n",
       " ('do', 'the'),\n",
       " ('the', 'same'),\n",
       " ('same', 'for'),\n",
       " ('for', 'liberal'),\n",
       " ('liberal', 'views'),\n",
       " ('views', 'is'),\n",
       " ('is', 'it'),\n",
       " ('it', 'crazy'),\n",
       " ('crazy', 'if'),\n",
       " ('if', 'i'),\n",
       " ('i', 'wash'),\n",
       " ('wash', 'or'),\n",
       " ('or', 'wipe'),\n",
       " ('wipe', 'my'),\n",
       " ('my', 'groceries'),\n",
       " ('groceries', 'off'),\n",
       " ('off', 'germs'),\n",
       " ('germs', 'are'),\n",
       " ('are', 'everywhere'),\n",
       " ('everywhere', 'is'),\n",
       " ('is', 'there'),\n",
       " ('there', 'such'),\n",
       " ('such', 'a'),\n",
       " ('a', 'thing'),\n",
       " ('thing', 'as'),\n",
       " ('as', 'dressing'),\n",
       " ('dressing', 'moderately'),\n",
       " ('moderately', 'and'),\n",
       " ('and', 'if'),\n",
       " ('if', 'so'),\n",
       " ('so', 'how'),\n",
       " ('how', 'is'),\n",
       " ('is', 'that'),\n",
       " ('that', 'different'),\n",
       " ('different', 'than'),\n",
       " ('than', 'dressing'),\n",
       " ('dressing', 'modestly'),\n",
       " ('modestly', 'is'),\n",
       " ('is', 'it'),\n",
       " ('it', 'just'),\n",
       " ('just', 'me'),\n",
       " ('me', 'or'),\n",
       " ('or', 'have'),\n",
       " ('have', 'you'),\n",
       " ('you', 'ever'),\n",
       " ('ever', 'been'),\n",
       " ('been', 'in'),\n",
       " ('in', 'this'),\n",
       " ('this', 'phase'),\n",
       " ('phase', 'wherein'),\n",
       " ('wherein', 'you'),\n",
       " ('you', 'became'),\n",
       " ('became', 'ignorant'),\n",
       " ('ignorant', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'people'),\n",
       " ('people', 'you'),\n",
       " ('you', 'once'),\n",
       " ('once', 'loved'),\n",
       " ('loved', 'completely'),\n",
       " ('completely', 'disregarding'),\n",
       " ('disregarding', 'their'),\n",
       " ('their', 'so'),\n",
       " ('so', 'you'),\n",
       " ('you', 'get'),\n",
       " ('get', 'to'),\n",
       " ('to', 'have'),\n",
       " ('have', 'something'),\n",
       " ('something', 'go'),\n",
       " ('go', 'your'),\n",
       " ('your', 'way'),\n",
       " ('way', 'and'),\n",
       " ('and', 'feel'),\n",
       " ('feel', 'temporarily'),\n",
       " ('temporarily', 'at'),\n",
       " ('at', 'ease'),\n",
       " ('ease', 'how'),\n",
       " ('how', 'did'),\n",
       " ('did', 'things'),\n",
       " ('things', 'change'),\n",
       " ('change', 'what'),\n",
       " ('what', 'can'),\n",
       " ('can', 'you'),\n",
       " ('you', 'say'),\n",
       " ('say', 'about'),\n",
       " ('about', 'feminism'),\n",
       " ('feminism', 'how'),\n",
       " ('how', 'were'),\n",
       " ('were', 'the'),\n",
       " ('the', 'calgary'),\n",
       " ('calgary', 'flames'),\n",
       " ('flames', 'founded'),\n",
       " ('founded', 'what'),\n",
       " ('what', 'is'),\n",
       " ('is', 'the'),\n",
       " ('the', 'dumbest'),\n",
       " ('dumbest', 'yet'),\n",
       " ('yet', 'possibly'),\n",
       " ('possibly', 'true'),\n",
       " ('true', 'explanation'),\n",
       " ('explanation', 'for'),\n",
       " ('for', 'trump'),\n",
       " ('trump', 'being'),\n",
       " ('being', 'elected'),\n",
       " ('elected', 'can'),\n",
       " ('can', 'we'),\n",
       " ('we', 'use'),\n",
       " ('use', 'our'),\n",
       " ('our', 'external'),\n",
       " ('external', 'hard'),\n",
       " ('hard', 'disk'),\n",
       " ('disk', 'as'),\n",
       " ('as', 'a'),\n",
       " ('a', 'os'),\n",
       " ('os', 'as'),\n",
       " ('as', 'well'),\n",
       " ('well', 'as'),\n",
       " ('as', 'for'),\n",
       " ('for', 'data'),\n",
       " ('data', 'the'),\n",
       " ('the', 'data'),\n",
       " ('data', 'be'),\n",
       " ('be', 'affected'),\n",
       " ('affected', 'i'),\n",
       " ('i', 'am'),\n",
       " ('am', 'living'),\n",
       " ('living', 'at'),\n",
       " ('at', 'home'),\n",
       " ('home', 'and'),\n",
       " ('and', 'have'),\n",
       " ('have', 'no'),\n",
       " ('no', 'boyfriend'),\n",
       " ('boyfriend', 'i'),\n",
       " ('i', 'would'),\n",
       " ('would', 'love'),\n",
       " ('love', 'a'),\n",
       " ('a', 'boyfriend'),\n",
       " ('boyfriend', 'and'),\n",
       " ('and', 'my'),\n",
       " ('my', 'own'),\n",
       " ('own', 'home'),\n",
       " ('home', 'how'),\n",
       " ('how', 'can'),\n",
       " ('can', 'i'),\n",
       " ('i', 'progress'),\n",
       " ('progress', 'my'),\n",
       " ('my', 'situation'),\n",
       " ('situation', 'what'),\n",
       " ('what', 'do'),\n",
       " ('do', 'you'),\n",
       " ('you', 'know'),\n",
       " ('know', 'about'),\n",
       " ('about', 'bram'),\n",
       " ('bram', 'fischer'),\n",
       " ('fischer', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'rivonia'),\n",
       " ('rivonia', 'trial'),\n",
       " ('trial', 'how'),\n",
       " ('how', 'difficult'),\n",
       " ('difficult', 'is'),\n",
       " ('is', 'it'),\n",
       " ('it', 'to'),\n",
       " ('to', 'find'),\n",
       " ('find', 'a'),\n",
       " ('a', 'good'),\n",
       " ('good', 'instructor'),\n",
       " ('instructor', 'to'),\n",
       " ('to', 'take'),\n",
       " ('take', 'a'),\n",
       " ('a', 'class'),\n",
       " ('class', 'near'),\n",
       " ('near', 'you'),\n",
       " ('you', 'have'),\n",
       " ('have', 'you'),\n",
       " ('you', 'licked'),\n",
       " ('licked', 'the'),\n",
       " ('the', 'skin'),\n",
       " ('skin', 'of'),\n",
       " ('of', 'a'),\n",
       " ('a', 'corpse'),\n",
       " ('corpse', 'do'),\n",
       " ('do', 'you'),\n",
       " ('you', 'think'),\n",
       " ('think', 'amazon'),\n",
       " ('amazon', 'will'),\n",
       " ('will', 'adopt'),\n",
       " ('adopt', 'an'),\n",
       " ('an', 'in'),\n",
       " ('in', 'house'),\n",
       " ('house', 'approach'),\n",
       " ('approach', 'to'),\n",
       " ('to', 'manufacturing'),\n",
       " ('manufacturing', 'similar'),\n",
       " ('similar', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'tesla'),\n",
       " ('tesla', 'or'),\n",
       " ('or', 'space'),\n",
       " ('space', 'x'),\n",
       " ('x', 'business'),\n",
       " ('business', 'models'),\n",
       " ('models', 'how'),\n",
       " ('how', 'many'),\n",
       " ('many', 'baronies'),\n",
       " ('baronies', 'might'),\n",
       " ('might', 'exist'),\n",
       " ('exist', 'within'),\n",
       " ('within', 'a'),\n",
       " ('a', 'county'),\n",
       " ('county', 'palatine'),\n",
       " ('palatine', 'how'),\n",
       " ('how', 'i'),\n",
       " ('i', 'know'),\n",
       " ('know', 'whether'),\n",
       " ('whether', 'a'),\n",
       " ('a', 'girl'),\n",
       " ('girl', 'had'),\n",
       " ('had', 'done'),\n",
       " ('done', 'sex'),\n",
       " ('sex', 'before'),\n",
       " ('before', 'sex'),\n",
       " ('sex', 'with'),\n",
       " ('with', 'me'),\n",
       " ('me', 'how'),\n",
       " ('how', 'do'),\n",
       " ('do', 'i'),\n",
       " ('i', 'become'),\n",
       " ('become', 'a'),\n",
       " ('a', 'fast'),\n",
       " ('fast', 'learner'),\n",
       " ('learner', 'both'),\n",
       " ('both', 'in'),\n",
       " ('in', 'my'),\n",
       " ('my', 'professional'),\n",
       " ('professional', 'career'),\n",
       " ('career', 'and'),\n",
       " ('and', 'in'),\n",
       " ('in', 'my'),\n",
       " ('my', 'personal'),\n",
       " ('personal', 'life'),\n",
       " ('life', 'has'),\n",
       " ('has', 'the'),\n",
       " ('the', 'united'),\n",
       " ('united', 'states'),\n",
       " ('states', 'become'),\n",
       " ('become', 'the'),\n",
       " ('the', 'largest'),\n",
       " ('largest', 'dictatorship'),\n",
       " ('dictatorship', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'world'),\n",
       " ('world', 'what'),\n",
       " ('what', 'is'),\n",
       " ('is', 'the'),\n",
       " ('the', 'strangest'),\n",
       " ('strangest', 'phenomenon'),\n",
       " ('phenomenon', 'you'),\n",
       " ('you', 'know'),\n",
       " ('know', 'of'),\n",
       " ('of', 'have'),\n",
       " ('have', 'witnessed'),\n",
       " ('witnessed', 'or'),\n",
       " ('or', 'have'),\n",
       " ('have', 'generated'),\n",
       " ('generated', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'area'),\n",
       " ('area', 'of'),\n",
       " ('of', 'electronics'),\n",
       " ('electronics', 'that'),\n",
       " ('that', 'has'),\n",
       " ('has', 'no'),\n",
       " ('no', 'explanation'),\n",
       " ('explanation', 'in'),\n",
       " ('in', 'terms'),\n",
       " ('terms', 'of'),\n",
       " ('of', 'modern'),\n",
       " ('modern', 'physics'),\n",
       " ('physics', 'should'),\n",
       " ('should', 'i'),\n",
       " ('i', 'leave'),\n",
       " ('leave', 'my'),\n",
       " ('my', 'friends'),\n",
       " ('friends', 'and'),\n",
       " ('and', 'find'),\n",
       " ('find', 'new'),\n",
       " ('new', 'ones'),\n",
       " ('ones', 'can'),\n",
       " ('can', 'you'),\n",
       " ('you', 'make'),\n",
       " ('make', 'amazon'),\n",
       " ('amazon', 'alexa'),\n",
       " ('alexa', 'trigger'),\n",
       " ('trigger', 'events'),\n",
       " ('events', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'browser'),\n",
       " ('browser', 'why'),\n",
       " ('why', 'have'),\n",
       " ('have', 'two'),\n",
       " ('two', 'democracies'),\n",
       " ('democracies', 'never'),\n",
       " ('never', 'ever'),\n",
       " ('ever', 'went'),\n",
       " ('went', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'full'),\n",
       " ('full', 'fledged'),\n",
       " ('fledged', 'war'),\n",
       " ('war', 'what'),\n",
       " ('what', 'stops'),\n",
       " ('stops', 'them'),\n",
       " ('them', 'how'),\n",
       " ('how', 'can'),\n",
       " ('can', 'i'),\n",
       " ('i', 'top'),\n",
       " ('top', 'cbse'),\n",
       " ('cbse', 'in'),\n",
       " ('in', 'months'),\n",
       " ('months', 'what'),\n",
       " ('what', 'should'),\n",
       " ('should', 'i'),\n",
       " ('i', 'know'),\n",
       " ('know', 'before'),\n",
       " ('before', 'visiting'),\n",
       " ('visiting', 'mcleodganj'),\n",
       " ('mcleodganj', 'and'),\n",
       " ('and', 'doing'),\n",
       " ('doing', 'the'),\n",
       " ('the', 'triund'),\n",
       " ('triund', 'trek'),\n",
       " ('trek', 'how'),\n",
       " ('how', 'do'),\n",
       " ('do', 'modern'),\n",
       " ('modern', 'military'),\n",
       " ('military', 'submarines'),\n",
       " ('submarines', 'reduce'),\n",
       " ('reduce', 'noise'),\n",
       " ('noise', 'to'),\n",
       " ('to', 'achieve'),\n",
       " ('achieve', 'stealth'),\n",
       " ('stealth', 'which'),\n",
       " ('which', 'babies'),\n",
       " ('babies', 'are'),\n",
       " ('are', 'more'),\n",
       " ('more', 'sweeter'),\n",
       " ('sweeter', 'to'),\n",
       " ('to', 'their'),\n",
       " ('their', 'parents'),\n",
       " ('parents', 'dark'),\n",
       " ('dark', 'skin'),\n",
       " ('skin', 'babies'),\n",
       " ('babies', 'or'),\n",
       " ('or', 'light'),\n",
       " ('light', 'skin'),\n",
       " ('skin', 'babies'),\n",
       " ('babies', 'how'),\n",
       " ('how', 'can'),\n",
       " ('can', 'i'),\n",
       " ('i', 'remove'),\n",
       " ('remove', 'black'),\n",
       " ('black', 'heads'),\n",
       " ('heads', 'which'),\n",
       " ('which', 'are'),\n",
       " ('are', 'all'),\n",
       " ('all', 'over'),\n",
       " ('over', 'my'),\n",
       " ('my', 'nose'),\n",
       " ('nose', 'if'),\n",
       " ('if', 'lightsabers'),\n",
       " ('lightsabers', 'are'),\n",
       " ('are', 'created'),\n",
       " ('created', 'by'),\n",
       " ('by', 'individual'),\n",
       " ('individual', 'wielders'),\n",
       " ('wielders', 'does'),\n",
       " ('does', 'each'),\n",
       " ('each', 'saber'),\n",
       " ('saber', 'have'),\n",
       " ('have', 'unique'),\n",
       " ('unique', 'is'),\n",
       " ('is', 'anyone'),\n",
       " ('anyone', 'still'),\n",
       " ('still', 'using'),\n",
       " ('using', 'visual'),\n",
       " ('visual', 'basic'),\n",
       " ('basic', 'is'),\n",
       " ('is', 'it'),\n",
       " ('it', 'worth'),\n",
       " ('worth', 'learning'),\n",
       " ('learning', 'in'),\n",
       " ('in', 'would'),\n",
       " ('would', 'there'),\n",
       " ('there', 'be'),\n",
       " ('be', 'professional'),\n",
       " ('professional', 'jobs'),\n",
       " ('jobs', 'for'),\n",
       " ('for', 'visual'),\n",
       " ('visual', 'basic'),\n",
       " ('basic', 'programmers'),\n",
       " ('programmers', 'in'),\n",
       " ('in', 'what'),\n",
       " ('what', 'is'),\n",
       " ('is', 'sykes'),\n",
       " ('sykes', 'enterprises'),\n",
       " ('enterprises', 'all'),\n",
       " ('all', 'about'),\n",
       " ('about', 'is'),\n",
       " ('is', 'there'),\n",
       " ('there', 'any'),\n",
       " ('any', 'clear'),\n",
       " ('clear', 'relations'),\n",
       " ('relations', 'between'),\n",
       " ('between', 'the'),\n",
       " ('the', 'number'),\n",
       " ('number', 'of'),\n",
       " ('of', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'computational'),\n",
       " ('computational', 'performances'),\n",
       " ('performances', 'and'),\n",
       " ('and', 'requirements'),\n",
       " ('requirements', 'in'),\n",
       " ('in', 'fea'),\n",
       " ('fea', 'or'),\n",
       " ('or', 'cfd'),\n",
       " ('cfd', 'analyses'),\n",
       " ('analyses', 'for'),\n",
       " ('for', 'ansys'),\n",
       " ('ansys', 'solutions'),\n",
       " ('solutions', 'in'),\n",
       " ('in', 'particular'),\n",
       " ('particular', 'why'),\n",
       " ('why', 'my'),\n",
       " ('my', 'package'),\n",
       " ('package', 'still'),\n",
       " ('still', 'is'),\n",
       " ('is', 'isc'),\n",
       " ('isc', 'since'),\n",
       " ('since', 'may'),\n",
       " ('may', 'and'),\n",
       " ('and', 'i'),\n",
       " ('i', 'do'),\n",
       " ('do', 'have'),\n",
       " ('have', 'updated'),\n",
       " ('updated', 'what'),\n",
       " ('what', 'does'),\n",
       " ('does', 'great'),\n",
       " ('great', 'wit'),\n",
       " ('wit', 'mean'),\n",
       " ('mean', 'in'),\n",
       " ('in', 'your'),\n",
       " ('your', 'experience'),\n",
       " ('experience', 'working'),\n",
       " ('working', 'with'),\n",
       " ('with', 'realtors'),\n",
       " ('realtors', 'what'),\n",
       " ('what', 'do'),\n",
       " ('do', 'you'),\n",
       " ('you', 'wish'),\n",
       " ('wish', 'realtors'),\n",
       " ('realtors', 'did'),\n",
       " ('did', 'better'),\n",
       " ('better', 'how'),\n",
       " ('how', 'do'),\n",
       " ('do', 'i'),\n",
       " ('i', 'get'),\n",
       " ('get', 'charge'),\n",
       " ('charge', 'by'),\n",
       " ('by', 'contact'),\n",
       " ('contact', 'do'),\n",
       " ('do', 'all'),\n",
       " ('all', 'public'),\n",
       " ('public', 'school'),\n",
       " ('school', 'teachers'),\n",
       " ('teachers', 'automatically'),\n",
       " ('automatically', 'get'),\n",
       " ('get', 'vacation'),\n",
       " ('vacation', 'whenever'),\n",
       " ('whenever', 'they'),\n",
       " ('they', 'ask'),\n",
       " ('ask', 'for'),\n",
       " ('for', 'it'),\n",
       " ('it', 'what'),\n",
       " ('what', 'is'),\n",
       " ('is', 'the'),\n",
       " ('the', 'role'),\n",
       " ('role', 'of'),\n",
       " ('of', 'technology'),\n",
       " ('technology', 'in'),\n",
       " ('in', 'using'),\n",
       " ('using', 'a'),\n",
       " ('a', 'resource'),\n",
       " ('resource', 'should'),\n",
       " ('should', 'i'),\n",
       " ('i', 'opt'),\n",
       " ('opt', 'jaypee'),\n",
       " ('jaypee', 'university'),\n",
       " ('university', 'guna'),\n",
       " ('guna', 'for'),\n",
       " ('for', 'mechanical'),\n",
       " ('mechanical', 'engineering'),\n",
       " ('engineering', 'where'),\n",
       " ('where', 'can'),\n",
       " ('can', 'i'),\n",
       " ('i', 'download'),\n",
       " ('download', 'microsoft'),\n",
       " ('microsoft', 'word'),\n",
       " ('word', 'for'),\n",
       " ('for', 'windows'),\n",
       " ('windows', 'in'),\n",
       " ('in', 'hungarian'),\n",
       " ('hungarian', 'what'),\n",
       " ('what', 'do'),\n",
       " ('do', 'i'),\n",
       " ('i', 'need'),\n",
       " ('need', 'to'),\n",
       " ('to', 'know'),\n",
       " ('know', 'about'),\n",
       " ('about', 'buying'),\n",
       " ('buying', 'a'),\n",
       " ('a', 'car'),\n",
       " ('car', 'in'),\n",
       " ('in', 'south'),\n",
       " ('south', 'africa'),\n",
       " ('africa', 'as'),\n",
       " ('as', 'an'),\n",
       " ('an', 'american'),\n",
       " ('american', 'as'),\n",
       " ('as', 'someone'),\n",
       " ('someone', 'who'),\n",
       " ('who', 'did'),\n",
       " ('did', 'enjoy'),\n",
       " ('enjoy', 'harry'),\n",
       " ('harry', 'potter'),\n",
       " ('potter', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'order'),\n",
       " ('order', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'phoenix'),\n",
       " ('phoenix', 'movie'),\n",
       " ('movie', 'can'),\n",
       " ('can', 'i'),\n",
       " ('i', 'at'),\n",
       " ('at', 'least'),\n",
       " ('least', 'enjoy'),\n",
       " ('enjoy', 'the'),\n",
       " ('the', 'book'),\n",
       " ('book', 'of'),\n",
       " ('of', 'it'),\n",
       " ('it', 'what'),\n",
       " ('what', 'is'),\n",
       " ('is', 'the'),\n",
       " ('the', 'writing'),\n",
       " ('writing', 'style'),\n",
       " ('style', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'book'),\n",
       " ('book', 'how'),\n",
       " ('how', 'to'),\n",
       " ('to', 'resist'),\n",
       " ('resist', 'prince'),\n",
       " ('prince', 'charming'),\n",
       " ('charming', 'by'),\n",
       " ('by', 'linda'),\n",
       " ('linda', 'kage'),\n",
       " ('kage', 'my'),\n",
       " ('my', 'mother'),\n",
       " ('mother', 'expects'),\n",
       " ('expects', 'me'),\n",
       " ('me', 'to'),\n",
       " ('to', 'memorize'),\n",
       " ('memorize', 'all'),\n",
       " ('all', 'her'),\n",
       " ('her', 'usernames'),\n",
       " ('usernames', 'and'),\n",
       " ('and', 'passwords'),\n",
       " ('passwords', 'how'),\n",
       " ('how', 'can'),\n",
       " ('can', 'i'),\n",
       " ('i', 'make'),\n",
       " ('make', 'her'),\n",
       " ('her', 'more'),\n",
       " ('more', 'responsible'),\n",
       " ('responsible', 'about'),\n",
       " ('about', 'them'),\n",
       " ('them', 'as'),\n",
       " ('as', 'i'),\n",
       " ('i', 'will'),\n",
       " ('will', 'be'),\n",
       " ('be', 'going'),\n",
       " ('going', 'to'),\n",
       " ('to', 'college'),\n",
       " ('college', 'in'),\n",
       " ('in', 'one'),\n",
       " ('one', 'year'),\n",
       " ('year', 'what'),\n",
       " ('what', 'is'),\n",
       " ('is', 'that'),\n",
       " ('that', 'movie'),\n",
       " ('movie', 'in'),\n",
       " ('in', 'which'),\n",
       " ('which', 'a'),\n",
       " ('a', 'kid'),\n",
       " ('kid', 'is'),\n",
       " ('is', 'fooled'),\n",
       " ('fooled', 'into'),\n",
       " ('into', 'thinking'),\n",
       " ('thinking', 'that'),\n",
       " ('that', 'germs'),\n",
       " ('germs', 'will'),\n",
       " ('will', 'kill'),\n",
       " ('kill', 'him'),\n",
       " ('him', 'and'),\n",
       " ('and', 'so'),\n",
       " ('so', 'he'),\n",
       " ('he', 'lives'),\n",
       " ('lives', 'in'),\n",
       " ('in', 'a'),\n",
       " ('a', 'bubble'),\n",
       " ('bubble', 'for'),\n",
       " ('for', 'most'),\n",
       " ('most', 'of'),\n",
       " ('of', 'his'),\n",
       " ('his', 'life'),\n",
       " ('life', 'but'),\n",
       " ('but', 'then'),\n",
       " ('then', 'decides'),\n",
       " ('decides', 'to'),\n",
       " ('to', 'travel'),\n",
       " ('travel', 'the'),\n",
       " ('the', 'world'),\n",
       " ('world', 'in'),\n",
       " ('in', 'a'),\n",
       " ('a', 'portable'),\n",
       " ('portable', 'bubble'),\n",
       " ('bubble', 'why'),\n",
       " ('why', 'most'),\n",
       " ('most', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'computer'),\n",
       " ('computer', 'science'),\n",
       " ('science', 'student'),\n",
       " ('student', 'buy'),\n",
       " ('buy', 'final'),\n",
       " ('final', 'year'),\n",
       " ('year', 'project'),\n",
       " ('project', 'from'),\n",
       " ('from', 'outside'),\n",
       " ('outside', 'rather'),\n",
       " ('rather', 'doing'),\n",
       " ('doing', 'it'),\n",
       " ('it', 'by'),\n",
       " ('by', 'own'),\n",
       " ('own', 'is'),\n",
       " ('is', 'our'),\n",
       " ('our', 'education'),\n",
       " ('education', 'system'),\n",
       " ('system', 'really'),\n",
       " ('really', 'that'),\n",
       " ('that', 'week'),\n",
       " ('week', 'what'),\n",
       " ('what', 'are'),\n",
       " ('are', 'some'),\n",
       " ('some', 'ways'),\n",
       " ('ways', 'to'),\n",
       " ('to', 'shorten'),\n",
       " ('shorten', 'your'),\n",
       " ('your', 'period'),\n",
       " ('period', 'and'),\n",
       " ('and', 'what'),\n",
       " ('what', 'are'),\n",
       " ('are', 'the'),\n",
       " ('the', 'risks'),\n",
       " ('risks', 'of'),\n",
       " ('of', 'doing'),\n",
       " ('doing', 'it'),\n",
       " ('it', 'why'),\n",
       " ('why', 'do'),\n",
       " ('do', 'we'),\n",
       " ('we', 'calead'),\n",
       " ('calead', 'leap'),\n",
       " ('leap', 'year'),\n",
       " ('year', 'how'),\n",
       " ('how', 'many'),\n",
       " ('many', 'days'),\n",
       " ('days', 'will'),\n",
       " ('will', 'it'),\n",
       " ('it', 'take'),\n",
       " ('take', 'to'),\n",
       " ('to', 'get'),\n",
       " ('get', 'rid'),\n",
       " ('rid', 'of'),\n",
       " ('of', 'spleen'),\n",
       " ('spleen', 'enlargement'),\n",
       " ('enlargement', 'which'),\n",
       " ('which', 'machine'),\n",
       " ('machine', 'learning'),\n",
       " ('learning', 'techniques'),\n",
       " ('techniques', 'can'),\n",
       " ('can', 'be'),\n",
       " ('be', 'used'),\n",
       " ('used', 'to'),\n",
       " ('to', 'extract'),\n",
       " ('extract', 'metadata'),\n",
       " ('metadata', 'font'),\n",
       " ('font', 'color'),\n",
       " ('color', 'size'),\n",
       " ('size', 'indentation'),\n",
       " ('indentation', 'and'),\n",
       " ('and', 'alignment'),\n",
       " ('alignment', 'of'),\n",
       " ('of', 'a'),\n",
       " ('a', 'word'),\n",
       " ('word', 'document'),\n",
       " ('document', 'file'),\n",
       " ('file', 'and'),\n",
       " ('and', 'can'),\n",
       " ('can', 'it'),\n",
       " ('it', 'be'),\n",
       " ('be', 'integrated'),\n",
       " ('integrated', 'with'),\n",
       " ('with', 'a'),\n",
       " ('a', 'web'),\n",
       " ('web', 'application'),\n",
       " ('application', 'does'),\n",
       " ('does', 'it'),\n",
       " ('it', 'work'),\n",
       " ('work', 'with'),\n",
       " ('with', 'girls'),\n",
       " ('girls', 'the'),\n",
       " ('the', 'way'),\n",
       " ('way', 'hitch'),\n",
       " ('hitch', 'will'),\n",
       " ('will', 'smith'),\n",
       " ('smith', 'asks'),\n",
       " ('asks', 'not'),\n",
       " ('not', 'to'),\n",
       " ('to', 'dance'),\n",
       " ('dance', 'too'),\n",
       " ('too', 'much'),\n",
       " ('much', 'why'),\n",
       " ('why', 'india'),\n",
       " ('india', 'act'),\n",
       " ('act', 'was'),\n",
       " ('was', 'so'),\n",
       " ('so', 'special'),\n",
       " ('special', 'are'),\n",
       " ('are', 'there'),\n",
       " ('there', 'any'),\n",
       " ('any', 'sports'),\n",
       " ('sports', 'that'),\n",
       " ('that', 'you'),\n",
       " ('you', 'do'),\n",
       " ('do', 'like'),\n",
       " ('like', 'how'),\n",
       " ('how', 'do'),\n",
       " ('do', 'dna'),\n",
       " ('dna', 'and'),\n",
       " ('and', 'rna'),\n",
       " ('rna', 'compare'),\n",
       " ('compare', 'and'),\n",
       " ('and', 'contrast'),\n",
       " ('contrast', 'someone'),\n",
       " ('someone', 'breaks'),\n",
       " ('breaks', 'into'),\n",
       " ('into', 'your'),\n",
       " ('your', 'house'),\n",
       " ('house', 'you'),\n",
       " ('you', 'shoot'),\n",
       " ('shoot', 'and'),\n",
       " ('and', 'kill'),\n",
       " ('kill', 'them'),\n",
       " ('them', 'they'),\n",
       " ('they', 'were'),\n",
       " ('were', 'armed'),\n",
       " ('armed', 'with'),\n",
       " ('with', 'only'),\n",
       " ('only', 'a'),\n",
       " ('a', 'knife'),\n",
       " ('knife', 'what'),\n",
       " ('what', 'happens'),\n",
       " ('happens', 'now'),\n",
       " ('now', 'how'),\n",
       " ('how', 'can'),\n",
       " ('can', 'i'),\n",
       " ('i', 'write'),\n",
       " ('write', 'a'),\n",
       " ('a', 'biography'),\n",
       " ('biography', 'about'),\n",
       " ('about', 'gianni'),\n",
       " ('gianni', 'versace'),\n",
       " ('versace', 'are'),\n",
       " ('are', 'extroverted'),\n",
       " ('extroverted', 'better'),\n",
       " ('better', 'and'),\n",
       " ('and', 'faster'),\n",
       " ('faster', 'at'),\n",
       " ('at', 'processing'),\n",
       " ('processing', 'and'),\n",
       " ('and', 'expelling'),\n",
       " ('expelling', 'information'),\n",
       " ('information', 'than'),\n",
       " ('than', 'introverts'),\n",
       " ('introverts', 'have'),\n",
       " ('have', 'you'),\n",
       " ('you', 'ever'),\n",
       " ('ever', 'been'),\n",
       " ('been', 'recognized'),\n",
       " ('recognized', 'at'),\n",
       " ('at', 'a'),\n",
       " ('a', 'place'),\n",
       " ('place', 'very'),\n",
       " ('very', 'far'),\n",
       " ('far', 'from'),\n",
       " ('from', 'your'),\n",
       " ('your', 'home'),\n",
       " ('home', 'why'),\n",
       " ('why', 'do'),\n",
       " ('do', 'price'),\n",
       " ('price', 'comparison'),\n",
       " ('comparison', 'websites'),\n",
       " ('websites', 'work'),\n",
       " ('work', 'well'),\n",
       " ('well', 'in'),\n",
       " ('in', 'financial'),\n",
       " ('financial', 'services'),\n",
       " ('services', 'does'),\n",
       " ('does', 'ragging'),\n",
       " ('ragging', 'happen'),\n",
       " ('happen', 'at'),\n",
       " ('at', 'nift'),\n",
       " ('nift', 'bangalore'),\n",
       " ('bangalore', 'why'),\n",
       " ('why', 'their'),\n",
       " ('their', 'are'),\n",
       " ('are', 'so'),\n",
       " ('so', 'many'),\n",
       " ('many', 'bad'),\n",
       " ('bad', 'reviews'),\n",
       " ('reviews', 'of'),\n",
       " ('of', 'bahubali'),\n",
       " ('bahubali', 'on'),\n",
       " ('on', 'imdb'),\n",
       " ('imdb', 'is'),\n",
       " ('is', 'swallowing'),\n",
       " ('swallowing', 'listerine'),\n",
       " ('listerine', 'dangerous'),\n",
       " ('dangerous', 'what'),\n",
       " ('what', 'are'),\n",
       " ('are', 'the'),\n",
       " ('the', 'theories'),\n",
       " ('theories', 'in'),\n",
       " ('in', 'critical'),\n",
       " ('critical', 'thinking'),\n",
       " ('thinking', 'what'),\n",
       " ('what', 'are'),\n",
       " ('are', 'the'),\n",
       " ('the', 'biggest'),\n",
       " ('biggest', 'problems'),\n",
       " ('problems', 'questions'),\n",
       " ('questions', 'doubts'),\n",
       " ('doubts', 'that'),\n",
       " ('that', 'you'),\n",
       " ('you', 'come'),\n",
       " ('come', 'across'),\n",
       " ('across', 'when'),\n",
       " ('when', 'trying'),\n",
       " ('trying', 'to'),\n",
       " ('to', 'choose'),\n",
       " ('choose', 'the'),\n",
       " ('the', 'paint'),\n",
       " ('paint', 'color'),\n",
       " ('color', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'room'),\n",
       " ('room', 'how'),\n",
       " ('how', 'can'),\n",
       " ('can', 'i'),\n",
       " ('i', 'get'),\n",
       " ('get', 'cheap'),\n",
       " ('cheap', 'flights'),\n",
       " ('flights', 'in'),\n",
       " ('in', 'edinburgh'),\n",
       " ('edinburgh', 'what'),\n",
       " ('what', 'is'),\n",
       " ('is', 'china'),\n",
       " ('china', 'new'),\n",
       " ('new', 'chick'),\n",
       " ('chick', 'how'),\n",
       " ('how', 'do'),\n",
       " ('do', 'i'),\n",
       " ('i', 'send'),\n",
       " ('send', 'large'),\n",
       " ('large', 'picture'),\n",
       " ('picture', 'files'),\n",
       " ('files', 'through'),\n",
       " ('through', 'an'),\n",
       " ('an', 'email'),\n",
       " ('email', 'why'),\n",
       " ('why', 'does'),\n",
       " ('does', 'ebay'),\n",
       " ('ebay', 'allow'),\n",
       " ('allow', 'the'),\n",
       " ('the', 'sale'),\n",
       " ('sale', 'of'),\n",
       " ('of', 'wwii'),\n",
       " ('wwii', 'purple'),\n",
       " ('purple', 'heart'),\n",
       " ('heart', 'medals'),\n",
       " ('medals', 'even'),\n",
       " ('even', 'though'),\n",
       " ('though', 'they'),\n",
       " ('they', 'have'),\n",
       " ...]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these bigrams include end of one question and start of another\n",
    "list(bigrams(full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [s for s in train['question_text'][:100] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.Text(temp[1:100]).collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 1306122 samples and 1306122 outcomes>\n"
     ]
    }
   ],
   "source": [
    "f_dist = FreqDist([train['question_text'])\n",
    "print(f_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_dist.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vect = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_dtm = vect.fit_transform(train.question_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of lanuages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_list = set([langid.classify(s)[0] for s in train['question_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_of_questions</th>\n",
       "      <th>no_of_insincere</th>\n",
       "      <th>no_of_sincere</th>\n",
       "      <th>%_insinere</th>\n",
       "      <th>null_score</th>\n",
       "      <th>no_of_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>basic_stats</th>\n",
       "      <td>1306122.0</td>\n",
       "      <td>80810.0</td>\n",
       "      <td>1225312.0</td>\n",
       "      <td>6.187018</td>\n",
       "      <td>0.93813</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             no_of_questions  no_of_insincere  no_of_sincere  %_insinere  \\\n",
       "basic_stats        1306122.0          80810.0      1225312.0    6.187018   \n",
       "\n",
       "             null_score  no_of_lang  \n",
       "basic_stats     0.93813          84  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some are questions in different lanuages, some are questions regarding different lanuages.\n",
    "basic_stats['no_of_lang'] = len(lang_list)\n",
    "basic_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prelim Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.question_text\n",
    "y = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<979591x167408 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11321413 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "raw_train_dtm = vect.transform(X_train)\n",
    "raw_test_dtm = vect.transform(X_test)\n",
    "raw_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9352168404977179\n",
      "test score: 0.9342788280438917\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(raw_train_dtm, y_train)\n",
    "print('train score:', nb.score(raw_train_dtm, y_train))\n",
    "print('test score:', nb.score(raw_test_dtm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove 'English' stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<979591x167099 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5741332 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer( stop_words='english')\n",
    "vect.fit(X_train)\n",
    "raw_train_dtm = vect.transform(X_train)\n",
    "raw_test_dtm = vect.transform(X_test)\n",
    "raw_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9388193644082071\n",
      "test score: 0.9375036367144314\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(raw_train_dtm, y_train)\n",
    "print('train score:', nb.score(raw_train_dtm, y_train))\n",
    "print('test score:', nb.score(raw_test_dtm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<979591x2617452 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22271663 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit(X_train)\n",
    "raw_train_dtm = vect.transform(X_train)\n",
    "raw_test_dtm = vect.transform(X_test)\n",
    "raw_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9612521960695841\n",
      "test score: 0.9483540613295521\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nb = MultinomialNB()\n",
    "nb.fit(raw_train_dtm, y_train)\n",
    "print('train score:', nb.score(raw_train_dtm, y_train))\n",
    "print('test score:', nb.score(raw_test_dtm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9612521960695841\n",
      "test score: 0.9483540613295521\n",
      "Wall time: 6min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dct = DecisionTreeClassifier(max_depth=20)\n",
    "# dct.fit(raw_train_dtm, y_train)\n",
    "# print('train score:', nb.score(raw_train_dtm, y_train))\n",
    "# print('test score:', nb.score(raw_test_dtm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9612521960695841\n",
      "test score: 0.9483540613295521\n",
      "Wall time: 20min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dct = DecisionTreeClassifier(max_depth=40)\n",
    "# dct.fit(raw_train_dtm, y_train)\n",
    "# print('train score:', nb.score(raw_train_dtm, y_train))\n",
    "# print('test score:', nb.score(raw_test_dtm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<979591x8587254 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 32284653 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,3))\n",
    "vect.fit(X_train)\n",
    "raw_train_dtm = vect.transform(X_train)\n",
    "raw_test_dtm = vect.transform(X_test)\n",
    "raw_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9786757942855743\n",
      "test score: 0.9465594384606668\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(raw_train_dtm, y_train)\n",
    "print('train score:', nb.score(raw_train_dtm, y_train))\n",
    "print('test score:', nb.score(raw_test_dtm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<979591x79069 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11230041 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df = 2)\n",
    "vect.fit(X_train)\n",
    "raw_train_dtm = vect.transform(X_train)\n",
    "raw_test_dtm = vect.transform(X_test)\n",
    "raw_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9269725834557484\n",
      "test score: 0.9244635271995615\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(raw_train_dtm, y_train)\n",
    "print('train score:', nb.score(raw_train_dtm, y_train))\n",
    "print('test score:', nb.score(raw_test_dtm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<979591x167271 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11318243 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df = 1e3)\n",
    "vect.fit(X_train)\n",
    "raw_train_dtm = vect.transform(X_train)\n",
    "raw_test_dtm = vect.transform(X_test)\n",
    "raw_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9353669031258964\n",
      "test score: 0.9338347660712153\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(raw_train_dtm, y_train)\n",
    "print('train score:', nb.score(raw_train_dtm, y_train))\n",
    "print('test score:', nb.score(raw_test_dtm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
